<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><meta charSet="utf-8"/><meta name="robots" content="follow, index"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><link rel="preload" href="/_next/static/css/ed18997ddc36a503.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ed18997ddc36a503.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-59c5c889f52620d6.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-10779d2dcf90bfb3.js" defer=""></script><script src="/_next/static/chunks/pages/_app-6c8f3d4aed99a8b2.js" defer=""></script><script src="/_next/static/chunks/281-2503d455e5b415a8.js" defer=""></script><script src="/_next/static/chunks/pages/posts-7a35b4b6b302693e.js" defer=""></script><script src="/_next/static/O0wjkE3Jvkz89Vmzt_XB3/_buildManifest.js" defer=""></script><script src="/_next/static/O0wjkE3Jvkz89Vmzt_XB3/_ssgManifest.js" defer=""></script></head><body class="bg-white text-gray-700 antialiased"><div id="__next"><div class="flex flex-col h-screen "><header class="py-8 lg:px-10 "><div class="px-2 md:px-8 relative"><nav class="flex space-x-6 text-zinc-400 tracking-wide items-center justify-between "><a class="text-xl ml-4 " href="/"><div class="flex flex-row gap-1 items-center content-center "><img alt="profile logo" loading="lazy" width="48" height="48" decoding="async" data-nimg="1" class="mr-2" style="color:transparent" srcSet="https://aadiimages.imgix.net//aaditya-profile.png?w=48&amp;q=75 1x, https://aadiimages.imgix.net//aaditya-profile.png?w=96&amp;q=75 2x" src="https://aadiimages.imgix.net//aaditya-profile.png?w=96&amp;q=75"/> <span class="font-bold text-black">Aaditya</span> <span>Chapagain</span></div></a><div class="grow h-full"> </div><div class="hidden lg:flex space-x-6 text-zinc-400 tracking-wide items-center justify-between "><a class=" undefined" href="/">About Me</a><a class="text-black font-semibold" href="/posts">Logs</a><a class=" undefined" href="/projects">Projects</a><a class=" undefined" href="/sapiens">Thoughts</a><a class=" undefined" href="/resume">Resume</a><a class=" undefined" href="/contact">Contact</a></div><div class="flex lg:hidden border rounded-md p-2 mr-3 hover:bg-stone-200 "><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="font-size:24px;color:black"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div></nav></div></header><div class="p-4 lg:p-8  inline-block grow "><main class="py-8 px-8 lg:px-0 "><div class="container max-w-3xl m-auto px-4"><article class="mb-10"><a class="text-lg leading-6 font-bold" href="/posts/serverless-stacks-benefits">Serverless stacks and Its benefits</a><p>Benefits of using serverless stacks.</p><div class="text-gray-400"><time>6 days</time></div></article><article class="mb-10"><a class="text-lg leading-6 font-bold" href="/posts/distributed-training-with-slurm-on-gcp">Distributed Training of Deep Learning model with Slurm on GCP</a><p>Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.</p><div class="text-gray-400"><time>2 years</time></div></article><article class="mb-10"><a class="text-lg leading-6 font-bold" href="/posts/wandb-your-machine-learning-project">Wandb Your machine learning project.</a><p>Wandb Tool for visualizing and tracking your machine learning experiments better than tensorboard.</p><div class="text-gray-400"><time>3 years</time></div></article><article class="mb-10"><a class="text-lg leading-6 font-bold" href="/posts/speech-signal-processing-using-python">Speech Signal Processing using python</a><p>Signal Processing and Speech Recognition using python</p><div class="text-gray-400"><time>3 years</time></div></article><article class="mb-10"><a class="text-lg leading-6 font-bold" href="/posts/testing-in-go-nutshell">Testing Go Program in Nutshell.</a><p>Testing Framework in Go is extremely simple and minimal</p><div class="text-gray-400"><time>3 years</time></div></article></div></main></div><div class="static inset-x-0 bottom-0 content-center py-6 "><div class="flex flex-auto px-6 text-slate-400 gap-1.5 font-semibold "><div class="grow h-16 "> </div><div class="flex flex-col gap-1.5 items-center content-between "><div class="flex flex-auto items-center gap-2 "><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="copyright" class="svg-inline--fa fa-copyright " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="font-size:20px"><path fill="currentColor" d="M256 48a208 208 0 1 1 0 416 208 208 0 1 1 0-416zm0 464A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM199.4 312.6c-31.2-31.2-31.2-81.9 0-113.1s81.9-31.2 113.1 0c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9c-50-50-131-50-181 0s-50 131 0 181s131 50 181 0c9.4-9.4 9.4-24.6 0-33.9s-24.6-9.4-33.9 0c-31.2 31.2-81.9 31.2-113.1 0z"></path></svg><span>   2023  </span><span>Aaditya Chapagain</span></div><div class="ml-2 ">All Rights Reserved</div></div><div class="grow h-16 "> </div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"allPosts":[{"slug":"serverless-stacks-benefits","title":"Serverless stacks and Its benefits","date":"2023-03-05 14:00","modified":"2023-03-05 16:00","category":"Blog","summary":"Benefits of using serverless stacks.","tags":"serverless, aws, azure, serverless stacks, infrastructure","authors":["Aaditya Chapagain"],"status":"published","content":"\nBefore deep diving into the serverless stacks provided by AWS , we need to know what serverless really means.\n\n## What is serverless ?\n\nServerless is a cloud-native development model that allows developers to build and run applications without having to manage servers. The servers still gonna be involving while building serverless, but they are abstracted away from app development. A cloud provider like ( AWS, GCP \u0026 AZURE ) handles the routine work of provisioning, maintaining , and scaling the server infrastructure. Developers can simply package their code in containers for deployment.\n\nOnce deployed, serverless apps respond to demand and automatically scale up and downs as needed. Serverless architectures from cloud providers are usually metered on-demand through an event-driven execution model. As a result, **when a serverless function is sitting idle, it doesn’t cost anything**.\n\n## What are the services provided by aws serverless architecture ?\n\n[![serverless stack aws](/images/blog/serverlessstack.png)](https://aadiimages.imgix.net/images/blog/serverlessstack.png)\n\nIt provides many back end tasks like computing, databases, storage, processing and many more, this in result , allows the user to focus on his program and allows him to innovate.\n\n- Compute using lambda\n\n- Storage using Amazon S3\n\n- Database using Amazon Dynamodb\n\n- API proxy using amazon API Gateway\n\n- Application Integration using Amazon SNS\n\n- Orchestration using Step Functions\n\n- Analytics using Amazon Kinesis\n\n- Security \u0026 Access Control using its identity and Access Management\n\n## Pros of AWS Serverless Computing\n\n- Servers need no attention for installation and maintenance.\n\n- Payment is as per the throughput, making it value for money.\n\n- You can choose appropriate settings as per your products requirements, thus paying only for the functions you use.\n\n- Serverless have in-built availability and fault tolerance. User need not architect for these capabilities since the services running the application provide them by default.\n\n- Write code and deploy with just few steps, and it will be available to the world within few minutes. Thus no need to put any effort into or for creating and managing servers.\n\n## Cons of AWS Serverless Computing\n\n- [Serverless Architecture](https://aws.amazon.com/lambda/serverless-architectures-learn-more/) excecutes commands and functions on temporarily created container. So if a client performs few tasks on your app, the serverless architecture will create a temporary container and will destroy it as soon as the client is finished performing tasks, this results in delays which are also known as cold start.\n\n- As serverless architecture is based on the temporarily created containers, the usable memory is thus limited hindering the processes that require a lot of processing.\n\n- Another issue with Lambda is that it decides which third-party apps can be used to work on it, thus giving up a lot of control over your application.\n\n- Monitoring and Debugging are quite restrictive to what the vendor provides. It is fundamental with Lambda too. It lacks proper operational tools for monitoring and debugging.\n\n- Lack of local testing options.\n"},{"slug":"distributed-training-with-slurm-on-gcp","title":"Distributed Training of Deep Learning model with Slurm on GCP","date":"2020-09-15 12:00","modified":"2020-09-15 12:00","category":"Blog","summary":"Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python, slurm, distributed training, High performance computing, HPC, parallel processing, tensorflow, Pytorch, DL, Language model,","authors":["Aaditya Chapagain"],"status":"published","content":"\nRecently, I was working on Big Machine Learning project. The task was to pretraining Large Machine learning models (with parameter in the range of several Billion ). And Normal training approch didn't work ( obviously ).With 8 GPU Volta core machines, it would take several months to complete just 1 epcoh of training, that's the point when i think of distributed training. I was using gcp ( google cloud ) for training models and found out that google already have support for High Performance Computing with Slurm . You can find Minimal working example on slurm from google codelabs here [https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine](https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine).\n\nThrough this blog, I will try to explain what is HPC? , Why HPC ?, how can we train large Deep Learning models with slurm.\n\n# What is HPC ?\n\nHigh Performance Computing (HPC) is the use of supercomputers and parallel processing techniques for solving complex mathematical and computational problems. HPC technology primarily focuses on developing parallel processing algorithms and systems by incorporating both administration and parallel computational techniques. HPC is typically used for solving advanced problems that require a lot time .\n\n# Why HPC ?\n\nWhen you have loads of data and its processing takes really long time, then the approch **divide et impera** comes at hand.\n\nWith HPC, we can divide any job so that every node processes different partitions of the data in parallel, speeding up the execution time.\n\n# Slurm Workload Manager on GCP\n\nTo make computing with slurm easier in GCP, Google and SchedMD ( Slurm's Creators ) joined forces and as a result, we can run a Slurm cluster on Google Cloud Platform. We don't have to worry about the parallel computing techniques since slurm takes care of that and GCP takes care of setting up a cluster and providing resources.\nBasic architectural diagram of a stand-alone Slurm Cluster in Google Cloud Platform.\n\n[![Slum Architecture in GCP](/images/slurm-archi.png)](https://ibb.co/SPhjRyM)\n\nAs we can see in above pictures, Slurm cluster contains three types of nodes: **login**, **controller** and **Compute** node.\n\n- **Login Node** serves as an interface for the user: user should communicate with the cluster exclusively through the login node (starting the job, requiring resources, ...)\n- **Controller Node** manages resources and job scheduling for the user.\n- **Compute Node** executes the job.\n\n# Setting Up a Slurm Cluster on GCP\n\nBefore describing the setup, let us explain in short how does GCP implement a slurm cluster.\n\nIn GCP, a cluster is realized as a **deployment**. A deployment is an instantiation of a set of resources that are defined in a configuration. A deployment can contain a number of resources, across a variety of Google Cloud Platform services. When you create a deployment, Deployment Manager creates all of the described resources in the respective Google Cloud Platform APIs.\n\nThis brings us to the cluster's nodes. Each node in a cluster is actually a **Virtual Machine**.When a deployment is created, three new virtual machines appear in \"VM instances\" page, under \"Compute Engine\". Those VMs are login instances, Controller instances, and compute image instance.\n\nCompute Instance is a bit trickey part. One thing to notice is that deployment does not create compute instance,but exactly one compute image instance even if you request more compute nodes in your cluster. So, if a user requests 10 compute nodes for the cluster, those 10 virtual machines will not be immediately instantiated with the cluster deployment. Here's what is happening. These compute instances are created in the later step when you run a job and request the number of nodes for the job. Then the compute nodes will be allocated, and they will appear in the \"VM Instances\" page. Shortly after the job is completed, these virtual machines will be deallocated and will disappear from the list. This way a user gets new compute VMs everytime. The fact that deployment create compute image instance rather than compute nodes directly is that, you might not be using compute node all the time and creating compute nodes unnecessarily might affect your billing, so , slurm will create new compute nodes and use compute image instance as a templete to dynamically create new instance during running jobs, so that you will be billed for exact time period your compute node will run.\n\nBelow you can see visual representation of the described process:\n\n[![Slum Architecture in GCP](/images/slurm-hpc.png)](https://ibb.co/mFrGDy3)\n\nFinally, let's head to the cluster setup. In this blog post, we will setup Slurm cluster for training Deep Learning Model with several nodes.Customize the information so that they will suit your needs:\n\n1. Launch Google Cloud Shell\n2. Check that you already authenticated and that the project is already set to your **PROJECT_ID**:\n\n```bash\n$ gcloud auth list\n\nCredentialed accounts:\n\u003cyour email\u003e\n\n$ gcloud config list project\n[core]\nproject = \u003cPROJECT_ID\u003e\n\n```\n\n1. Clone git repository that contains the Slurm Google Cloud Platform deployment-manager files:\n\n```bash\n$ git clone https://github.com/SchedMD/slurm-gcp.git\n\n```\n\n2. Switch to the Slurm deployment configuration directory:\n\n```bash\n$ cd slurm-gcp\n```\n\n3. Configure the Slurm Deployment YAML file. Provide information that suits your needs. There are plenty more parameters available, they can be found in SchedMD's GitHub repository. Below is the script that was sufficient for my needs.\n\n```yaml\n\n# [START cluster_yaml]\nimports:\n- path: slurm.jinja\n\nresources:\n- name: slurm-cluster\ntype: slurm.jinja\nproperties:\n    cluster_name            : slurm-job\n\n    zone                    : us-central1-b\n    controller_machine_type : n1-standard-2\n    controller_disk_type      : pd-standard\n    controller_disk_size_gb   : 50\n    external_controller_ip    : True\n\n    login_machine_type        : n1-standard-2\n    login_disk_type           : pd-standard\n    login_disk_size_gb        : 50\n    external_login_ips        : True\n\n    compute_image_machine_type  : n1-standard-2\n    compute_image_disk_type   : pd-standard\n    compute_image_disk_size_gb: 200\n    external_compute_ips      : False\n\n    partitions :\n    - name           : gpu\n        machine_type   : n1-standard-16\n        max_node_count : 10\n        zone           : us-central1-b\n        # cpu_platform           : Intel Skylake\n        preemptible_bursting   : True\n        compute_disk_type      : pd-standard\n        compute_disk_size_gb   : 200\n        gpu_type               : nvidia-tesla-v100\n        gpu_count              : 1\n\n#  [END cluster_yaml]\n\n```\n\n4. In the Cloud shell Session, execute the following command from the slurm-gcp folder:\n\n```bash\n\u003e gcloud deployment-manager deployments create slurm-deployment  --config  slurm-cluster.yaml\n```\n\n    This command creates a deployment  named slurm-deployment. The operation can take few minutes to complete.\n\n5. Verify the deployment ( Navigation menu --\u003e Deployment Manager)\n6. Verify the cluster's instances ( Navigation menu --\u003e Compute Engine --\u003e VM Instances) There should be login , controller, and compute image instances. Compute image instance will live for small amount of time for setting up compute images, after that it will be off. and Compute Instances show up only when you allocate them for the sbatch job. they disappear shortly the job is completed.\n7. Log in to login instance .\n\n   While logging in to the login instances if _Slurm is currently being installed/configured in the background._ Don't install any packages during that time, as that might disrupt the installation process of slurm. Wait for around 10 for slurm to be installed fully and you can log in to login instances and do whatever you want.\n\n# Setting up Deep Learning Environment inside slurm cluster\n\n1. SSH to login instance in your slurm cluster.\n2. After sucessfully installing slurm on cluster, ( /home , /apps ) directory of login instances will be shared with all other instances like controller and compute instances. So that all deep learning and python binaries must be installed inside `/home` or `/apps` directory.\n3. Installing python inside /home:\n\n```bash\ncd ~\n\nsudo yum -y update; sudo yum groupinstall \"Development Tools\"\n\nsudo yum -y install openssl-devel bzip2-devel libffi-devel\n\nmkdir tmp;cd tmp; wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz\n\ntar zxvf Python-3.8.3.tgz; cd Python-3.8.3\n\n./configure --enable-optimizations --prefix=$HOME/opt/python-3.8\n\nmake\n\nmake altinstall\n\nvi ~/.bashrc\n(add line: export PATH=$HOME/opt/python-3.8/bin:$PATH) at the end of .bashrc\n\nsource ~/.bashrc\n```\n\n```\n\u003e which python3.8\n(output should be: /home/\u003cusername\u003e/opt/python-3.8/bin/python3)\n\n# Now Lets install pytorch\n\n\u003e curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n\u003e  python3.8 get-pip.py\n\n\u003e python3.8 -m pip install torch==1.5.0\n\n```\n\n4. Installing CUDA toolkits and Nvidia-drivers\n\n- Download the latest Nvidia CUDA [repository package](https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/) cuda-repo-rhel7-\\*.rpm.\n\n```bash\n\u003e cd ~\n\u003e wget https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-10.2.89-1.x86_64.rpm\n```\n\n- Install the CUDA repository package. This will enable CUDA repository on your CentOS 7 Linux system:\n\n```bash\n\u003e sudo rpm -i cuda-repo-*.rpm\n```\n\n- Install cuda package from nvidia repository. Pytorch 1.5 works with cuda 10.2, so lets install cuda 10.2. Below command will install cuda and nvidia-drivers inside `/usr/local/cuda-10.2` directory.\n\n```bash\n\u003e sudo yum install cuda-10-2\n```\n\n- By Now we know that only `/home` and `/apps` directory were shared across all other instances. So lets copy cuda directory from `/usr/local/` to `~/opt/cuda`\n\n```bash\n\u003e mkdir ~/opt/cuda\n\u003e cp -r /usr/local/cuda-10.2/* ~/opt/cuda/\n```\n\n- Export cuda path to Nvidia CUDA binary executables. Open the ~/.bashrc using your preferred text editor and add the following two lines\n\n```bash\nexport PATH=$HOME/opt/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=$HOME/opt/cuda/lib64:$LD_LIBRARY_PATH\n```\n\n- Now Re-login or execute your updated ~/.bashrc file:\n\n```bash\n\u003e source ~/.bashrc\n```\n\n- Now Confirm the CUDA installation:\n\n```bash\n\u003e nvcc --version\n\u003e nvidia-smi #( doesn't work as we dont have gpu in login instances )\n```\n\n\u003e By Now your slurm system is ready and can run any training script using `sbatch` scheduling. Below I will discuss on how to compile nvidia-apex library. It takes me 2 days just to make nvidia-apex work on slurm cluster.If you follow my above steps exaclty, I am sure you will be able to install nvidia apex library inside your slurm cluster without any problems.\n\n# Installing Nvidia-Apex library inside your slurm cluster ( Optional )\n\nIf you don't know about Apex library, you can visit Nvidia-Apex [github page](https://github.com/NVIDIA/apex) to know more about Apex library. It is a set of utilites to help training model in mixed precision mode. It provide more numerical stable layer norm operations during training model in mixed precision mode, So that you can have both faster and stable training.\n\nFor `Nvidia-apex` to sucessfully compile, You need to upgrade your gcc --version to 7.3.0 .\n\n```bash\n\u003e sudo yum install centos-release-scl\n\u003e sudo yum install devtoolset-7-gcc*\n\u003e scl enable devtoolset-7 bash\n\u003e which gcc\n\u003e gcc --version\n(above bash script must output gcc version as 7.3.0)\n```\n\nNow We will compile and install Apex-library inside python3.8\n\n```bash\n\n\u003e git clone https://github.com/NVIDIA/apex\n\u003e cd apex\n\u003e export TORCH_CUDA_ARCH_LIST=\"6.0;7.0\"\n\u003e CUDA_HOME=$HOME/opt/cuda python3.8 -m pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n```\n\nIts takes around 4 - 6 minutes to compile and install apex.\n\n# Distributed Training with pytorch and slurm\n\nI will not give details on how to setup codebase in pytorch for distributed training, Its a huge topic and might need another blog post for it only. Meanwhile, You can learn more about distributed training with pytorch [here](https://pytorch.org/tutorials/intermediate/dist_tuto.html). After you done setting up distributed training codebase in pytorch. You can start training using sbatch scripts.\n\n## Running the training Job Using SBATCH Script\n\nAfter we have done the cluster setup, preparing deep learning envs and building of the model, the last step is to finally run a job, i.e. to start the training. That is easily done by running sbatch script, which is basically a customized shell script. It effectively has two parts. The first part of the script is specific for the Slurm, it specifies the parameters for the Slurm job scheduler using the SBATCH command. The second part consists of bash (or some other shell) commands that you would normally run in terminal.\n\nBelow you will see demo SBATCH script (You need to modify sbatch script according to your needs).\n\n\u003e inside run_training.sh file.\n\n```bash\n#!/bin/sh\n #SBATCH --job-name=distributed_training\n  #SBATCH --output=slurm_logs/slrm_stdout.%j\n  #SBATCH --error=slurm_logs/slrm_stderr.%j\n  #SBATCH --partition=gpu\n  ## make sure we don't clobber log files if jobs get restarted\n  #SBATCH --open-mode=append\n  #SBATCH --nodes=2\n  #SBATCH --time=24:00:00\n  ## make sure we are told about preempts, and jobs running out of time, 60s beforehand\n  #SBATCH --signal=USR1@60\n  #SBATCH --cpus-per-task=5\n  ## srun forks ntasks_per_node times on each node\n  #SBATCH --ntasks-per-node=1\n  #SBATCH --mem=200G\n  #SBATCH --gpus-per-node=1\n\npython3.8 train.py \u003cYour training args here\u003e\n```\n\nExecute the sbatch script using the sbatch command line:\n\n```bash\n\u003e sbatch run_training.sh\n```\n\nRunning sbatch will return a Job ID for the scheduled job, for example:\n\n```\nSubmitted batch job 37\n```\n\nTo keep track of the job’s state, run squeue and to keep track of the cluster’s state, run sinfo:\n\n```bash\n\u003e squeue\n\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n3 gpu hostname \u003cusername\u003e CF 0:11 4 slurm-job-compute-0-[0-1]\n\n\u003e sinfo\n\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST\ngpu* up infinite 4 mix# slurm-job-compute-0-[0-1]\ngpu* up infinite 6 idle~ slurm-job-compute-0-[2-7]\n```\n\n# Summary\n\nIn this blog post we learned how to setup slurm cluster in GCP ( which is super easy ), setup own deeplearning environment from installing python, pytorch to compiling apex from source and finally run training using sbatch script and keeping track of job state using `sinfo`, `squeue` and `scontrol`.\n\n# REFERENCES\n\n1. [https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine](https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine)\n2. [https://cloud.google.com/deployment-manager/docs/deployments](https://cloud.google.com/deployment-manager/docs/deployments)\n3. [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n4. [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)\n"},{"slug":"wandb-your-machine-learning-project","title":"Wandb Your machine learning project.","date":"2020-09-02 12:00","modified":"2020-09-02 12:00","category":"Blog","summary":"Wandb Tool for visualizing and tracking your machine learning experiments better than tensorboard.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python","authors":["Aaditya Chapagain"],"status":"published","content":"\nHave you ever worked on large Machine learning projects or research where you have to manage many experimentations ? often during large projects or experiments you have to log every nits and bits of your Machine Learning training. Some of us might already be there.\nRecently I was also working on large Machine learning Projects and it was very hard for me to track every experiments logs, visualization, experiments. There were lots of them and It would take lots of time to prepare presentation's of my experiments results to my peers and that's when I learned about wandb.\n\nWandb is API created by [Weights \u0026 Biases](https://www.wandb.com/) to collect, manage and visualize all your Machine learning experiments all at one place. Ohh, wait you must be wandering that **tensorboard** can also do these things right ?\n\nWell, there are lots of things which makes **wandb** a straight winner.\n\n1. Visually aesthetics UI.\n2. More manageable and customizable visualization and experiment tracking.\n3. Super easy to use API.\n4. Analyze system usage metrics alongside runs.\n5. Collaborate with team members\n6. Run parameter sweeps\n7. Keep records of experiments available forever\n\nI think the last point in above list is super cool, which is not feasible to do with tensorboard i.e You have to write huge amount of code by yourself to log these system wide metrics into tensorboard, but with wandb you can do this with just one line of code. and\n\nLets Dive into wandb API.\n\n# Installation\n\nInstalling `wandb` is very easy, just run below command in terminal and you are good to go.\n\n```bash\npip install wandb\n\npip install wandb --upgrade\n```\n\nOne thing I noticed during installing wandb is that, if we run only `pip install wandb` it will sometimes only install wrapper class of wandb. So, upgrading wandb library after installing it , worked for me.\n\n# Integration with your python code\n\nYou can use `wandb` with any deeplearning framework, either it is pytorch or Tensorflow.But, first you need to have account on [Weights \u0026 Biases](https://www.wandb.com/). After you create account on [Weights \u0026 Biases](https://www.wandb.com/) , you can get your wandb api keys from [setting](https://app.wandb.ai/settings). After you get your keys, you just need to write couple of code to integrate your deeplearning system with wandb logger.\n\n## Login\n\nFirst you need to login into your wandb using wandb API KEYS. you can login programetically or from terminal, but login using terminal is advised.\n\n```bash\n# This is secret and shouldn't be checked into version control\nexport WANDB_API_KEY=$YOUR_API_KEY\n\nwandb login\n```\n\nTo Programatically login you can use following code:\n\n```python\nWANDB_API_KEYS = \"\u003cyour api keys here\u003e\"\n\nimport wandb\n\nwandb.login(WANDB_API_KEYS) # if you don't have $WANDB_API_KEYS in your env variable (Not Advised)\n\nwandb.login() # if $WANDB_API_KEYS is already been set.\n```\n\n## Initializing wandb ( wandb.init )\n\nBelow you will see simple wandb integration examples with keras.\n\n```python\n\n# initialize wandb with your project name and optionally with configutations.\nwandb.init(project='demo-keras-integration', name = 'first_run'\n           config={\n              \"learning_rate\": 0.005,\n              \"epochs\": 25,\n              \"batch_size\": 64,\n              \"loss_function\": \"sparse_categorical_crossentropy\",\n              \"architecture\": \"CNN\",\n              \"dataset\": \"CIFAR-10\",\n           }, anonymous='never')\nconfig = wandb.config\n\n# Initialize model like you usually do.\ntf.keras.backend.clear_session()\nmodel = Model()\nmodel.summary()\n\n# compile model like you usually do.\n# notice use of config.\noptimizer = tf.keras.optimizers.Adam(config.learning_rate)\nmodel.compile(optimizer, config.loss_function, metrics=['acc'])\n\n```\n\n`project` is name of your project and your project might have different run, so `name` parameter will distinguish one run from other.\nIf you run above code and get back to your wandb dashboard it will new project called `demo-keras-integration` and with `first_run` in it.\n\n[![wandb Init](/images/wandb-init.png)](https://ibb.co/hDjwC6s)\n\nAbove init configuration will create new run everytime you call `wandb.init` But sometimes that not what you want, If your training is preemptible and might takes days or months to complete then you can resume from previous logged metrics by providing `resume = True` parameter to `wandb.init` and you will also need to set unique id, to distinguish one run from another. Below code will resume your logs with current run in your wandb dashboard even after you run `init` multiple times.\n\n```python\n\nwandb.init(project='demo-keras-integration', name = 'first_run', resume= True,\n            id = 'my_first_run'\n           config={\n              \"learning_rate\": 0.005,\n              \"epochs\": 25,\n              \"batch_size\": 64,\n              \"loss_function\": \"sparse_categorical_crossentropy\",\n              \"architecture\": \"CNN\",\n              \"dataset\": \"CIFAR-10\",\n           }, anonymous='never')\n```\n\n## Train with Wandb callback\n\nwandb made easy to log your model metrics into your project space by providing various callback function to log your metrics directly into wandb dashboard, without writing extra code.\n\n```python\n\nfrom wandb.keras import WandbCallback\n\n# train with our favorite model.fit\n# notice WandbCallback used as a regular callback\n# notice the use of config\n_ = model.fit(x_train, y_train,\n          epochs=config.epochs,\n          batch_size=config.batch_size,\n          validation_data=(x_test, y_test),\n          callbacks=[WandbCallback()])\n\n```\n\nIf you are working on images and need a way to log your correctly classified and misclassified sample images, you can also do so using wandb. Below you will see the examples of it.\n\n```python\n\n# in order to get prediction on small subset of images.\nval_images, val_labels = x_test[:32], y_test[:32]\n\n# train with our favorite model.fit\n# notice WandbCallback used as a regular callback\n# notice that we are passing in some arguments as well\n# notice the use of config\n_ = model.fit(x_train, y_train,\n          epochs=config.epochs,\n          batch_size=config.batch_size,\n          validation_data=(x_test, y_test),\n          callbacks=[WandbCallback(data_type='image',\n                                   training_data=(val_images, val_labels),\n                                   labels=CLASS_NAMES)])\n```\n\n[![logging images with wandb](/images/wand-log-images.png)](https://ibb.co/ZcbvNP9)\n\n## Log Custom metrics with wandb.log\n\nYou can always log custom metrics and extra information using wandb.log .\n\n```python\nloss, accuracy = model.evaluate(x_test, y_test)\nprint('Test Error Rate: ', round((1-accuracy)*100, 2))\n\n# notice the use of wandb.log.\n# We can easiy pass in values as key-value pairs.\nwandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})\n\n```\n\n# wandb Dashboard\n\n[![Wandb dashboard](/images/wandb-dashboard.png)](https://ibb.co/tBZDJzW)\n\nYou can see all your logged metrics in your wandb project dashboard. Wandb Groups information into different sections like Overview, Charts ,logs, system, model and files.\n\n## Overview\n\n[![Wandb dashboard](/images/wandb-overview.png)](https://ibb.co/RvNFHTW)\n\nThe information in the `overview` section is pretty intuitive and self-explanatory. However, the Git Repository field and the Git State field are worthy of special mention. You can run the checkout command in the Git State field to pin down the exact code for reproducing the experiment. Under the hood, wandb tracks all the changes you made to the original repo, and save the \"diff\" files in a local directory. In this way, you can easily switch between different versions of the code without manually pushing the change to the remote repo.\n\n## Logs\n\nThe `logs` section shows the console output during the experiment. This is useful for debugging the performance of the model.\n\n[![Wandb dashboard](/images/wandb-logs.png)](https://ibb.co/hmKFC7H)\n\n## Systems\n\nTo me, the system section is where wandb really shines and separates itself from other options such as TensorBoard. It is a centralized place to track system utilization during the experiment. There are in total of 8 graphs displayed in this section. These graphs give you insight into the training bottleneck and possible ways to uplift it. For example, below are the diagrams of the experiment:\n\n[![Wandb dashboard](/images/wandb-system.png)](https://ibb.co/VNGQbyQ)\n\n## Tensorboard\n\nwandb also provide options for people who love tensorboard. You can directly sync wandb with tensorboard by just setting `sync_tensorboard = True` in your `wandb.init`. So, that every information that is logged into tensorboard will also be logged into wandb.\n\n```python\n\nwandb.init(\n    name = '\u003cyour run name\u003e',\n    project = 'your project name',\n    config = '\u003cyour config\u003e',\n    sync_tensorboard = True)\n```\n\n# Summary\n\nWe discuss how to integrate wandb in any deeplearning framework using python for inspecting the efficiency of training jobs. We also looked into other aspects of wandb that makes it so much unique than other training logging software like tensorboard.\n\nTo learn more about wandb, check out their website: [https://www.wandb.com/](https://www.wandb.com/).\n"},{"slug":"speech-signal-processing-using-python","title":"Speech Signal Processing using python","date":"2020-08-11 08:00","modified":"2020-08-11 08:00","category":"Blog","summary":"Signal Processing and Speech Recognition using python","tags":"signal processing, speech recognition, ASR, machine learning, Deep Learning.","authors":["Aaditya Chapagain"],"status":"published","content":"\n## Table of contents\n\nSpeech processing is very first phase in any speech system either it is speech recognition system or speaker Diarization or something else. Speech processing plays an important role in speech system to extract vocal features i.e identify the components of the audio signal that are good for identifying the linguistic content and discarding all other stuff which carries information like background noise, emotion etc.\n\nIn this post we will learn very important mathematical concept about speech processing in any speech system and implement the mathematics in python.\n\nMel Frequency Cepstral Coefficents (MFCCs) and Filter Banks are a feature widely used in automatic speech and speaker recognition. But Filter Banks is more popluar Nowadays due to its robustness on mapping vocal features.Computing filter banks and MFCCs involve somewhat same procedure, where in both cases filter banks are computed and with a few more extra steps MFCCs can be obtained.\n\nLet's get started with loading speech signal with python. I will be using python 3.6 for this post.\n\n```python\n\nimport numpy as np\nfrom scipy.io import wavfile\nfrom scipy.fftpack import dct\nfrom matplotlib import pyplot as plt\n\nsample_rate, signal = wavfile.read('../audio/mfcc.wav')\n\nsignal = signal[0:int(10* sample_rate)]\nTime = np.linspace(0, len(signal) / sample_rate, num=len(signal))\n\nplt.plot(Time, signal)\n```\n\nThe raw signal has the following from in the time domain:\n[![raw signal image of speech](/images/spectral-image-speech.png)](https://ibb.co/nPvnTGc)\n\n## Pre-Emphasis\n\nThe first step is to apply a pre-emphesis filter on signal to amplify the high frequencies. A pre-emphesis filter is useful in several ways:\n\n- balance frequency spectrum since high frequencies usually have smaller magnitudes compared to lower frequencies\n- avoid numerical problems during fourier operation\n- Might also improve the signal to Noise Ratio (SNR)\n\nThe pre-emphesis filter can be applied to a signal x using the first order filter in the following equation:\n\n$$ y(t) = x(t) - \\alpha x(t-1) $$\n\nwhich can be easily implemented using the following line, where typical values for the filter coefficeint ( $\\alpha$ ) are 0.95 to 0.97,\n`pre_empasis = 0.97`\n\n```python\n\npre_emphasis = 0.97\nemphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n\n```\n\nPre-emphasis has a modest effect in modern systems, mainly because most of the motivations for the pre-emphasis filter can be achieved using mean normalization except for avoiding the Fourier transfrom numerical issues which should not be a problem in modern FFT implementations.\n\nThe signal after pre-emphasis has the following effect in orignal signal.\n\n[![before and after pre-emphesis](/images/before-after-preemphesis.png)](https://ibb.co/Jqn0x33)\n\n## Framing\n\nAfter pre-emphasis, we need to split the signal into short-time frames. The rationale behind this step is that frequencies in a signal change over time, so in most cases it dosen't make sense to do the Fourier Transform across the entier signal in that we would lose the frequency contours of the signal over time. To avoid that, we can safely assume that frequencies in a signal are stationary over a very short period of time. Therefore, by doing a Fourier transform over this short-time frame, we can obtain a good approximation of the frequency contours of the signal by concatenating adjacent frames.\n\nTypical frame sizes in speech processing rnage from 20 ms to 40 ms with 50% (+/- 10%) overlap between conseutive frames.Popular settings are 25 ms for the frame size, frame_size = 0.025 and a 10 ms stride (15 ms overlap), frame_stride = 0.01\n\n```python\n\nframe_size = 0.025\nframe_stride = 0.01\n\nframe_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\nsignal_length = len(emphasized_signal)\nframe_length = int(round(frame_length))\nframe_step = int(round(frame_step))\nnum_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n\npad_signal_length = num_frames * frame_step + frame_length\nz = np.zeros((pad_signal_length - signal_length))\npad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n\nindices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\nframes = pad_signal[indices.astype(np.int32, copy=False)]\n\n```\n\n## Window\n\nAfter slicing the signal into frames, we apply a window function such as the Hamming window to each frame. A Hamming window has the following form:\n$$ w[n] = 0.54 − 0.46 cos ( \\frac{2πn}{N − 1} ) $$\nwhere,  0 $\\leq$ n $\\leq$ N-1,N is the window length.\n\nThere are several reasons why we need to apply a window function to the frames, notably to counteract the assumption made by the FFT that the data is infinite and to reduce spectral leakage.\n\n````python\n\nframes *= np.hamming(frame_length)```\n# frames *= 0.54 - 0.46 * np.cos((2 * np.pi * n) / (frame_length - 1))  # Explicit Implementation **\n````\n\n## Fourier-Transfrom and Power Spectrum\n\nWe can now do an N-point FFT on each frame to calculate the frequency spectrum, which is also called Short-Time-Fourier-Transfrom (STFT), where N is typically 256 or 512, `NFFT = 512`; and then compute the power spectrum (Periodogram) using the following equation:\n$$P = \\frac{|FFT(x_i)|^2}{N}$$\nwhere , $x_i$ is the $i^{th}$ frame of signal $x$. This can be easily imlemented with the following lines:\n\n```python\nNFFT = 512\n\nmag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\npow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n```\n\n## Filter Banks\n\nThe final step to computing filter banks is applying triangular filters, typically 40 filters, nfilt = 40 on a Mel-scale to the power spectrum to extract frequency bands. The Mel-Scale aims to mimic the non-linear human ear perception of sound, by being more discriminative at lower frequenceis and less discriminative at higher frequencies. We can convert between Hertz ($f$) and Mel ( $m$) using the following equations:\n$$ \\Large m = 2595 \\log\\_{10} (1 + \\frac{f}{700}) \\approx 1125 \\ln (1 + \\frac{f}{700} ) $$\n\n$$ \\Large f = 700 ( 10^{ \\frac{m}{2595} } -1)$$\n\nEach filter in the filter bank is triangular having a response of 1 at the center frequency and decrease linearly towards 0 till it reaches the center frequencies of the two adjacent filters where the response is 0. which can be modeled by the following equation:\n\n$$\nH_m(k) =\n  \\begin{cases}\n      \\ 0                                      \u0026 k \u003c f(m - 1) \\\\\n      \\\\\n      \\dfrac{k - f(m - 1)}{f(m) - f(m - 1)}  \u0026 f(m - 1) \\leq k \u003c f(m) \\\\\n      \\\\\n      1                                      \u0026 k = f(m) \\\\\n      \\\\\n      \\dfrac{f(m + 1) - k}{f(m + 1) - f(m)}  \u0026 f(m) \u003c k \\leq f(m + 1) \\\\\n      \\\\\n      0                                      \u0026 k \u003e f(m + 1) \\\\\n  \\end{cases}\n$$\n\n```python\nnfilt = 40\n\nlow_freq_mel = 0\nhigh_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\nmel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\nhz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\nbin = np.floor((NFFT + 1) * hz_points / sample_rate)\n\nfbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\nfor m in range(1, nfilt + 1):\n    f_m_minus = int(bin[m - 1])   # left\n    f_m = int(bin[m])             # center\n    f_m_plus = int(bin[m + 1])    # right\n\n    for k in range(f_m_minus, f_m):\n        fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n    for k in range(f_m, f_m_plus):\n        fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\nfilter_banks = np.dot(pow_frames, fbank.T)\nfilter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\nfilter_banks = 20 * np.log10(filter_banks)  # dB\n```\n\nAfter applying the filter bank to the power spectrum (peridogram) of the signal, we obtain the following spectrogram:\n\n```python\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\ncax = ax.matshow(\n    np.transpose(filter_banks),\n    interpolation=\"nearest\",\n    aspect=\"auto\",\n    cmap=plt.cm.afmhot_r,\n    origin=\"lower\",\n)\nfig.colorbar(cax)\nplt.title(\"Mel compression Spectrogram\")\nplt.show()\n```\n\n[![melspectrogram of speech signal](/images/mfcc-spectorgram.png)](https://ibb.co/xSyfhsV)\n\n## Mel-frequency cepstral Coecfficents (MFCCs)\n\nIt turns out that filter bank coefficients computed in the previous step are higly correlated, which could be problematic in some machine learning algorithms. Therefore, we can apply Discrete Cosine Transform(DCT) to decorrelate the filter bank coefficients and yield compressed representation of filter banks. Typically, for Automatic SPeech Recognition (ASR), the resulting cepstral coefficeints 2- 13 are retained and the rest are discareded; `num_ceps = 12`. The reasons for discarding the other coefficeints is that they represent fast changes in the filter bank coefficients and these fine details don't contribute to ASR.\n\n```python\nnum_ceps = 12\nmfcc = dct(filter_banks, type = 2, axis=1, norm=\"ortho\")[:,1: (num_ceps + 1)] # keep 2-13\n\n```\n\nOne may apply sinusoidal liftering to the MFCCs to de-emphasize higher MFCCs which has been claimed to improve speech recognition in noisy signals.\n\n```python\n\ncep_lifter = 22\n(nframes, ncoeff) = mfcc.shape\nn = np.arange(ncoeff)\nlift = 1 + (cep_lifter / 2) * np.sin(np.pi * n/ cep_lifter)\nmfcc *= lift\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\ncax = ax.matshow(\n    np.transpose(mfcc),\n    interpolation=\"nearest\",\n    aspect=\"auto\",\n    cmap=plt.cm.afmhot_r,\n    origin=\"lower\",\n)\nfig.colorbar(cax)\nplt.title(\"MFCC Spectrogram\")\nplt.show()\n```\n\n[![MFCC spectrogram of speech signal](/images/mel-spectrogram.png)](https://ibb.co/S7c5pnL)\n\n## Mean Normalization\n\nAs previously mentioned, to balance the spectrum and improve the Signal-to-Noise (SNR), we can simply substract the mean of each coefficeint from all frames,\n\n```python\nfilter_banks -= (np.mean(filter_banks, axis = 0) + 1e-8)\n```\n\nand similarly for MFCCs:\n\n```python\n\nmfcc -= (np.mean(mfcc, axis = 0) + 1e-8)\n\n```\n\nTo this point, the steps to compute filter banks and MFCCs were discussed in terms of their motivation and implementations.It is interesting to note that all steps needed to compute filter banks were motivated by the nature of the speech signal and the human perception of such signals.\n\nOn the Contrary, the extra steps needed to compute MFCCs were motivated by the limitation of some classifical machine learning algorithms.The Discrete cosine Transfrom (DCT) was needed to decorrelate filter banks coeffiicients, a process refer as whitening. In particular, MFCCs were very popular with Gaussian Mixture Models - Hidden Markov Models(GMM - HMM).\nBut with the advent of Deep learning in speech system, one might not need DCT.\n"},{"slug":"testing-in-go-nutshell","title":"Testing Go Program in Nutshell.","date":"2020-08-11 08:00","modified":"2020-08-11 08:00","category":"Blog","summary":"Testing Framework in Go is extremely simple and minimal","tags":"Go, Golang, Test, Testing, software development, Programming language, TDD","authors":["Aaditya Chapagain"],"status":"published","content":"\n## Table of contents\n\nTesting, by which we implicitly mean _automated testing_, is the practice of writing small programs that check that the code under test ( the production code ) behaved as expected for certain input pools, which is usually either carefully chosen to exercise certain features or randomized to ensure broad coverage.\n\nGo's approach to testing seems rather low-tech in comparison to some big language like java, c++. It relies on one command, `go test`, and a set of conventions for writing test functions that go test can run. The comparatively lightweight mechanism is effective for pure testing, and it extends naturally to benchmarks and systematic example for documentation. The best thing about writing test code in Go is , that test code is no different from code if we intend to implement API we are testing.We focus on short functions that focus on one part of the task.We have to be careful about boundary conditions. think about data structures, and reason about what results a computation should produce from suitable inputs. But is the same as writing ordinary Go code.\n\n# The GO test Tool\n\nTesting in Go starts with `go test` subcommand i.e. test driver for Go packages that are organized according to certain conventions. `go test` command specifically looks and execute files that ends with **\\_test.go** which are not part of the package when built by `go build `.\n\nThe Go test files contains three kinds of special functions :\n\n- A _test function_: Function which name starts with `Test`. This function exercises some program logic for correct behaviour; subcommand `go test` calls the test function and reports the result which is either `PASS` or `FAIL`.\n\n- A _benchmark function_: It has the name beginning with `Benchmark` and measures the performance of some operations; subcommand `go test` reports the mean execution time of the operation.\n\n- An _example function_: Its name starts with `Example` provides machine-checked documentation.\n\nThe `go test` tool scans the \\*\\_test.go files for these special functions, generates a temporary main package that calls them all in the proper way, builds and runs it, reports the result, and then cleans up.\n\n# Test Functions\n\nEach test file must import the testing package. Test functions has following signature :\n\n```go\nfunc TestName(t *testing.T) {\n  // ..\n}\n\n```\n\nTest function names must begin with Test; The optional suffix Name must begin with a capital letter:\n\n```go\nfunc TestSin(t *testing.T) { // ... }\nfunc TestCos(t *testing.T) { // ... }\n```\n\nThe t parameter provides methods for reporting test failures and logging additional information. Lets create a new package palindrome containing a single function IsPalindrome that reports whether a string a string reads same forward and backward.\n\n```go\npackage palindrome\n\nfunc IsPalidrome(s string) bool {\n  for i := range s {\n    if s[i] != s[len(s) - 1 - i] {\n      return false\n    }\n  }\n  return true\n}\n\n```\n\nIn the same directory, the palindrome_test.go contains two test functions named TestPalindrome and TestNonPalindrome. Each check that IsPalindrome gives the right answer for a single input and reports failures using t.Errorf:\n\n```go\npackage palindrome\n\nfunc TestPalindrome(t *testing.T) {\n  if !IsPalindrome(\"detartrated\") {\n    t.Errorf(`IsPalindrome(\"detartrated\") = false`)\n  }\n  if !IsPalindrome(\"kayak\") {\n    t.Errorf(`IsPalindrome(\"kayak\") = false`)\n  }\n}\n\nfunc TestNonPalindrome(t *testing.T) {\n  if IsPalindrome(\"palindrome\") {\n    t.Error(`IsPalindrome(\"palindrome\") = true`)\n  }\n}\n\n```\n\nNow,we already have test code to test our functionality, we can use go test command to run tests in go.\nRemember `go test palindrome` will run the package level test.For this to work you have to place your package in either `$GOROOT` or `$GOPATH` directory.\nIn our case we can test our functionlity by going into pacakge directory.\n\n```bash\n\u003e cd \u003cpath_to_project_dir\u003e/palindrome\n\u003e go test\n\nPASS\nok  \t\u003cpath_to_project_dir\u003e/palindrome\t0.001s\n```\n\nTo look into more details on test function, To see which one failed and which one succeed, we can use `-v` flag , which will prints the name and execution time of each test in the package.\n\n```bash\n\u003e go test -v\n\n=== RUN TestPalindrome\n--- PASS: TestPalindrome (0.00s)\n=== RUN TestNonPalindrome\n--- PASS: TestNonPalindrome (0.00s)\n=== RUN TestFrenchPalindrome\n--- FAIL: TestFrenchPalindrome (0.00s)\nword_test.go:28: IsPalindrome(\"été\") = false\n=== RUN TestCanalPalindrome\n--- FAIL: TestCanalPalindrome (0.00s)\nword_test.go:35: IsPalindrome(\"A man, a plan, a canal: Panama\") = false\nFAIL\nexit status 1\nFAIL  \u003cpath_to_project_dir\u003e/palindrome 0.001s\n\n```\n\nand the -run flag, whose argument is a regular expression, causes go test to run only those tests whose function name matches the patter:\n\n```bash\n\n\u003e go test -v -run=\"French|Canal\"\n=== RUN TestFrenchPalindrome\n--- FAIL: TestFrenchPalindrome (0.00s)\nword_test.go:28: IsPalindrome(\"été\") = false\n=== RUN TestCanalPalindrome\n--- FAIL: TestCanalPalindrome (0.00s)\nword_test.go:35: IsPalindrome(\"A man, a plan, a canal: Panama\") = false\nFAIL\nexit status 1\nFAIL  \u003cpath_to_project_dir\u003e/palindrome 0.001s\n\n```\n\n# Benchmark Function\n\nBenchmarking is the practice of measuring the performance of a program on a fixed workload. In Go, a benchmark function look like a test function, but with the\nBenchmark prefix and a *testing.B parameter that provides most of the same methods as a *testing.T, plus few extra related performance measurement.It also exposes\nan Integer field N, which specifies the number of times to perform the operation being measured.\n\nHere's a benchmark for IsPalindrome that calls it N times in a loop.\n\n```go\nfunc BenchmarkIsPalindrome(b *testing.B) {\n  for i := 0; i \u003c b.N; i++ {\n    IsPalindrome(\"A man, a plan, a canal: Panama\")\n  }\n}\n```\n\nUnlike tests, by default no benchmarks are run. The argument to the -bench flag selects which benchmark to run. It is regular expression matching the names of Benchmark functions, with a default value that matches none of the functions. The \".\" pattern causes it to match all benchmark functions in package.\n\n```bash\n\u003e go test -bench=.\ngoos: linux\ngoarch: amd64\nBenchmarkIsPalindrome-12    \t 4377900\t       301 ns/op\nPASS\nok  \t\u003cpath_to_project_dir\u003e/palindrome\t1.598s\n```\n\nThe benchmark name’s numeric suffix, 12 here , indicates the value of GOMAXPROCS, which is important for concurrent benchmarks.\n\nThe report tells us that each call to _IsPalindrome_ took about 0.301 microseconds, averaged over 4377900 runs. Since the benchmark runner initially has no idea how long the operation takes, it make some initial measuremetns using small values of N and then extrapolates to a value large enough for a stable timing measurement to be made.\n"}]},"__N_SSG":true},"page":"/posts","query":{},"buildId":"O0wjkE3Jvkz89Vmzt_XB3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>