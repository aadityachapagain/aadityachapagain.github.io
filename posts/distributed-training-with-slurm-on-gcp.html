<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="description" content="Tech, life, uprising , upbringing, futuristic logbook made with love."/><title>Distributed Training of Deep Learning model with Slurm on GCP<!-- --> | My awesome blog</title><meta name="next-head-count" content="4"/><meta charSet="utf-8"/><meta name="robots" content="follow, index"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><link rel="preload" href="/_next/static/css/5b42724419df19dd.css" as="style"/><link rel="stylesheet" href="/_next/static/css/5b42724419df19dd.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-ee7e63bc15b31913.js" defer=""></script><script src="/_next/static/chunks/framework-2c79e2a64abdb08b.js" defer=""></script><script src="/_next/static/chunks/main-ff44f97138765e5a.js" defer=""></script><script src="/_next/static/chunks/pages/_app-ec1e9e99cc312d2b.js" defer=""></script><script src="/_next/static/chunks/281-2503d455e5b415a8.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-394bf49cca78315a.js" defer=""></script><script src="/_next/static/0ig78gGTz-PJyHrhou7UP/_buildManifest.js" defer=""></script><script src="/_next/static/0ig78gGTz-PJyHrhou7UP/_ssgManifest.js" defer=""></script></head><body class="bg-white text-gray-700 antialiased"><div id="__next"><div class="flex flex-col h-screen "><header class="py-8 lg:px-10 "><div class="px-2 md:px-8 relative"><nav class="flex space-x-6 text-zinc-400 tracking-wide items-center justify-between "><a class="text-xl ml-4 " href="/"><div class="flex flex-row gap-1 items-center content-center "><img alt="profile logo" srcSet="https://aadiimages.imgix.net//aaditya-profile.png?w=48&amp;q=75 1x, https://aadiimages.imgix.net//aaditya-profile.png?w=96&amp;q=75 2x" src="https://aadiimages.imgix.net//aaditya-profile.png?w=96&amp;q=75" width="48" height="48" decoding="async" data-nimg="1" class="mr-2" loading="lazy" style="color:transparent"/> <span class="font-bold text-black">Aaditya</span> <span>Chapagain</span></div></a><div class="grow h-full"> </div><div class="hidden lg:flex space-x-6 text-zinc-400 tracking-wide items-center justify-between "><a class=" undefined" href="/">About Me</a><a class=" undefined" href="/posts">Logs</a><a class=" undefined" href="/projects">Projects</a><a class=" undefined" href="/sapiens">Thoughts</a><a class=" undefined" href="/resume">Resume</a><a class=" undefined" href="/contact">Contact</a></div><div class="flex lg:hidden border rounded-md p-2 mr-3 hover:bg-stone-200 "><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="bars" class="svg-inline--fa fa-bars " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" style="font-size:24px;color:black"><path fill="currentColor" d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg></div></nav></div></header><div class="p-4 lg:p-8 inline-block grow "><main class="py-8 px-8 lg:px-0 "><div class="container max-w-3xl m-auto px-4"><div><article><header><h1 class="text-4xl font-bold">Distributed Training of Deep Learning model with Slurm on GCP</h1><p class="mt-2 text-xl">Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.</p><time class="flex mt-2 text-gray-400">2 years ago</time></header><div class="prose mt-10 "><p>Recently, I was working on Big Machine Learning project. The task was to pretraining Large Machine learning models (with parameter in the range of several Billion ). And Normal training approch didn't work ( obviously ).With 8 GPU Volta core machines, it would take several months to complete just 1 epcoh of training, that's the point when i think of distributed training. I was using gcp ( google cloud ) for training models and found out that google already have support for High Performance Computing with Slurm . You can find Minimal working example on slurm from google codelabs here <a href="https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine">https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine</a>.</p>
<p>Through this blog, I will try to explain what is HPC? , Why HPC ?, how can we train large Deep Learning models with slurm.</p>
<h1 id="what-is-hpc-">What is HPC ?</h1>
<p>High Performance Computing  (HPC) is the use of supercomputers and parallel processing techniques for solving complex mathematical and computational problems. HPC technology primarily focuses on developing parallel processing algorithms and systems by incorporating both administration and parallel computational techniques. HPC is typically used for solving advanced problems that require a lot time .</p>
<h1 id="why-hpc-">Why HPC ?</h1>
<p>When you have loads of data and its processing takes really long time, then the approch <strong>divide et impera</strong> comes at hand.</p>
<p>With HPC, we can divide any job so that every node processes different partitions of the data in parallel, speeding up the execution time.</p>
<h1 id="slurm-workload-manager-on-gcp">Slurm Workload Manager on GCP</h1>
<p>To make computing with  slurm easier in GCP, Google and SchedMD ( Slurm's Creators ) joined forces and as a result, we can run a Slurm cluster on Google Cloud Platform. We don't have to worry about the parallel computing techniques since slurm takes care of that and GCP takes care of setting up a cluster and providing resources.
Basic architectural diagram of a stand-alone Slurm Cluster in Google Cloud Platform.</p>
<p><a href="https://ibb.co/SPhjRyM"><img src="/images/slurm-archi.png" alt="Slum Architecture in GCP"></a></p>
<p>As we can see in above pictures, Slurm cluster contains three types of nodes: <strong>login</strong>, <strong>controller</strong> and <strong>Compute</strong> node.</p>
<ul>
<li><strong>Login Node</strong> serves as an interface for the user: user should communicate with the cluster exclusively through the login node (starting the job, requiring resources, ...)</li>
<li><strong>Controller Node</strong> manages resources and job scheduling for the user.</li>
<li><strong>Compute Node</strong> executes the job.</li>
</ul>
<h1 id="setting-up-a-slurm-cluster-on-gcp">Setting Up a Slurm Cluster on GCP</h1>
<p>Before describing the setup, let us explain in short how does GCP implement a slurm cluster.</p>
<p>In GCP, a cluster is realized as a <strong>deployment</strong>. A deployment is an instantiation of a set of resources  that are defined in a configuration. A deployment  can contain a number of resources, across a variety of Google Cloud Platform services. When you create a deployment, Deployment Manager creates all of the described resources in the respective Google Cloud Platform APIs.</p>
<p>This brings us to the cluster's nodes. Each node in a cluster is actually a <strong>Virtual Machine</strong>.When a deployment is created, three new virtual machines appear in "VM instances" page, under "Compute  Engine". Those VMs are login instances, Controller instances, and compute image instance.</p>
<p>Compute Instance is a bit trickey part. One thing to notice is that deployment does not create compute instance,but exactly one compute image instance even if you request more compute nodes in your cluster. So, if a user requests 10 compute nodes for the cluster, those 10 virtual machines will not be immediately instantiated with the cluster deployment. Here's what is happening. These compute instances are created in the later step when you run a job and request the number of nodes for the job. Then the compute nodes will be allocated, and they will appear in the "VM Instances" page. Shortly after the job is completed, these virtual machines will be deallocated and will disappear from the list. This way a user gets new compute VMs everytime. The fact that deployment create compute image instance rather than compute nodes directly is that, you might not be using compute node all the time and creating compute nodes unnecessarily might affect your billing, so , slurm will create new compute nodes and use compute image instance as a templete to dynamically create new instance during running jobs, so that you will be billed for exact time period your compute node will run.</p>
<p>Below you can see visual representation of the described process:</p>
<p><a href="https://ibb.co/mFrGDy3"><img src="/images/slurm-hpc.png" alt="Slum Architecture in GCP"></a></p>
<p>Finally, let's head to the cluster setup. In this blog post, we will setup Slurm cluster for training Deep Learning Model with several nodes.Customize the information so that they will suit your needs:</p>
<ol>
<li>Launch Google Cloud Shell</li>
<li>Check that you already authenticated and that the project is already set to your <strong>PROJECT_ID</strong>:</li>
</ol>
<pre><code class="hljs language-bash">$ gcloud auth list

Credentialed accounts:
&#x3C;your email>

$ gcloud config list project
[core]
project = &#x3C;PROJECT_ID>

</code></pre>
<ol>
<li>Clone git repository that contains the Slurm Google Cloud Platform deployment-manager files:</li>
</ol>
<pre><code class="hljs language-bash">$ git <span class="hljs-built_in">clone</span> https://github.com/SchedMD/slurm-gcp.git

</code></pre>
<ol start="2">
<li>Switch to the Slurm deployment configuration directory:</li>
</ol>
<pre><code class="hljs language-bash">$ <span class="hljs-built_in">cd</span> slurm-gcp
</code></pre>
<ol start="3">
<li>Configure the Slurm Deployment YAML file. Provide information that suits your needs. There are plenty more parameters available, they can be found in SchedMD's GitHub repository. Below is the script that was sufficient for my needs.</li>
</ol>
<pre><code class="hljs language-yaml">
<span class="hljs-comment"># [START cluster_yaml]</span>
<span class="hljs-attr">imports:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">slurm.jinja</span>

<span class="hljs-attr">resources:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">slurm-cluster</span>
<span class="hljs-attr">type:</span> <span class="hljs-string">slurm.jinja</span>
<span class="hljs-attr">properties:</span>
    <span class="hljs-attr">cluster_name            :</span> <span class="hljs-string">slurm-job</span>

    <span class="hljs-attr">zone                    :</span> <span class="hljs-string">us-central1-b</span>
    <span class="hljs-attr">controller_machine_type :</span> <span class="hljs-string">n1-standard-2</span>
    <span class="hljs-attr">controller_disk_type      :</span> <span class="hljs-string">pd-standard</span>
    <span class="hljs-attr">controller_disk_size_gb   :</span> <span class="hljs-number">50</span>
    <span class="hljs-attr">external_controller_ip    :</span> <span class="hljs-literal">True</span>

    <span class="hljs-attr">login_machine_type        :</span> <span class="hljs-string">n1-standard-2</span>
    <span class="hljs-attr">login_disk_type           :</span> <span class="hljs-string">pd-standard</span>
    <span class="hljs-attr">login_disk_size_gb        :</span> <span class="hljs-number">50</span>
    <span class="hljs-attr">external_login_ips        :</span> <span class="hljs-literal">True</span>

    <span class="hljs-attr">compute_image_machine_type  :</span> <span class="hljs-string">n1-standard-2</span>
    <span class="hljs-attr">compute_image_disk_type   :</span> <span class="hljs-string">pd-standard</span>
    <span class="hljs-attr">compute_image_disk_size_gb:</span> <span class="hljs-number">200</span>
    <span class="hljs-attr">external_compute_ips      :</span> <span class="hljs-literal">False</span>

    <span class="hljs-attr">partitions :</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name           :</span> <span class="hljs-string">gpu</span>
        <span class="hljs-attr">machine_type   :</span> <span class="hljs-string">n1-standard-16</span>
        <span class="hljs-attr">max_node_count :</span> <span class="hljs-number">10</span>
        <span class="hljs-attr">zone           :</span> <span class="hljs-string">us-central1-b</span>
        <span class="hljs-comment"># cpu_platform           : Intel Skylake</span>
        <span class="hljs-attr">preemptible_bursting   :</span> <span class="hljs-literal">True</span>
        <span class="hljs-attr">compute_disk_type      :</span> <span class="hljs-string">pd-standard</span>
        <span class="hljs-attr">compute_disk_size_gb   :</span> <span class="hljs-number">200</span>
        <span class="hljs-attr">gpu_type               :</span> <span class="hljs-string">nvidia-tesla-v100</span>
        <span class="hljs-attr">gpu_count              :</span> <span class="hljs-number">1</span>

<span class="hljs-comment">#  [END cluster_yaml]</span>

</code></pre>
<ol start="4">
<li>In the Cloud shell Session, execute the following command from the slurm-gcp folder:</li>
</ol>
<pre><code class="hljs language-bash">> gcloud deployment-manager deployments create slurm-deployment  --config  slurm-cluster.yaml
</code></pre>
<pre><code>This command creates a deployment  named slurm-deployment. The operation can take few minutes to complete.
</code></pre>
<p>5. Verify the deployment ( Navigation menu -->  Deployment Manager)
6. Verify the cluster's instances ( Navigation menu --> Compute Engine --> VM Instances)  There should be login , controller, and compute image instances. Compute image instance will live for small amount of time for setting up compute images, after that it will be off. and Compute Instances show up only when you allocate them for the sbatch job. they disappear shortly the job is completed.</p>
<ol start="7">
<li>
<p>Log in to login instance .</p>
<p>While logging in to the login instances if <em>Slurm is currently being installed/configured in the background.</em> Don't install any packages  during that time, as that might disrupt the installation process of slurm. Wait for around 10 for slurm to be installed fully and you can log in to login instances and do whatever you want.</p>
</li>
</ol>
<h1 id="setting-up-deep-learning-environment-inside-slurm-cluster">Setting up Deep Learning Environment inside slurm cluster</h1>
<ol>
<li>SSH to login instance in your slurm cluster.</li>
<li>After sucessfully installing slurm on cluster,  ( /home , /apps ) directory of login instances will be shared with all other instances  like controller and compute instances. So that all deep learning  and python binaries must be installed inside <code>/home</code> or <code>/apps</code> directory.</li>
<li>Installing python inside /home:</li>
</ol>
<pre><code class="hljs language-bash"><span class="hljs-built_in">cd</span> ~

sudo yum -y update; sudo yum groupinstall <span class="hljs-string">"Development Tools"</span>

sudo yum -y install openssl-devel bzip2-devel libffi-devel

<span class="hljs-built_in">mkdir</span> tmp;<span class="hljs-built_in">cd</span> tmp; wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz

tar zxvf Python-3.8.3.tgz; <span class="hljs-built_in">cd</span> Python-3.8.3

./configure --enable-optimizations --prefix=<span class="hljs-variable">$HOME</span>/opt/python-3.8

make

make altinstall

vi ~/.bashrc
(add line: <span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$HOME</span>/opt/python-3.8/bin:<span class="hljs-variable">$PATH</span>) at the end of .bashrc 

<span class="hljs-built_in">source</span> ~/.bashrc
</code></pre>
<pre><code>> which python3.8
(output should be: /home/&#x3C;username>/opt/python-3.8/bin/python3)

# Now Lets install pytorch

> curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py

>  python3.8 get-pip.py

> python3.8 -m pip install torch==1.5.0

</code></pre>
<ol start="4">
<li>Installing CUDA toolkits and Nvidia-drivers</li>
</ol>
<ul>
<li>Download the latest Nvidia CUDA <a href="https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/">repository package</a> cuda-repo-rhel7-*.rpm.</li>
</ul>
<pre><code class="hljs language-bash">> <span class="hljs-built_in">cd</span> ~
> wget https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-10.2.89-1.x86_64.rpm
</code></pre>
<ul>
<li>Install the CUDA repository package. This will enable CUDA repository on your CentOS 7 Linux system:</li>
</ul>
<pre><code class="hljs language-bash">> sudo rpm -i cuda-repo-*.rpm
</code></pre>
<ul>
<li>Install cuda package from nvidia repository. Pytorch 1.5 works with cuda 10.2, so lets install cuda 10.2. Below command will install cuda and nvidia-drivers inside  <code>/usr/local/cuda-10.2</code> directory.</li>
</ul>
<pre><code class="hljs language-bash">> sudo yum install cuda-10-2
</code></pre>
<ul>
<li>By Now we know that only  <code>/home</code> and <code>/apps</code> directory were shared across all other instances. So lets copy cuda directory from <code>/usr/local/</code> to <code>~/opt/cuda</code></li>
</ul>
<pre><code class="hljs language-bash">> <span class="hljs-built_in">mkdir</span> ~/opt/cuda 
> <span class="hljs-built_in">cp</span> -r /usr/local/cuda-10.2/* ~/opt/cuda/
</code></pre>
<ul>
<li>Export cuda path to Nvidia CUDA binary executables. Open the ~/.bashrc using your preferred text editor and add the following two lines</li>
</ul>
<pre><code class="hljs language-bash"><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$HOME</span>/opt/cuda/bin:<span class="hljs-variable">$PATH</span>
<span class="hljs-built_in">export</span> LD_LIBRARY_PATH=<span class="hljs-variable">$HOME</span>/opt/cuda/lib64:<span class="hljs-variable">$LD_LIBRARY_PATH</span>
</code></pre>
<ul>
<li>Now Re-login or execute your updated ~/.bashrc file:</li>
</ul>
<pre><code class="hljs language-bash">> <span class="hljs-built_in">source</span> ~/.bashrc
</code></pre>
<ul>
<li>Now  Confirm the CUDA installation:</li>
</ul>
<pre><code class="hljs language-bash">> nvcc --version
> nvidia-smi <span class="hljs-comment">#( doesn't work as we dont have gpu in login instances )</span>
</code></pre>
<blockquote>
<p>By Now your slurm system is ready and can run any training script using <code>sbatch</code> scheduling. Below I will discuss on how to compile nvidia-apex library. It takes me 2 days just to make nvidia-apex work on slurm cluster.If you follow my above steps exaclty, I am sure you will be able to install nvidia apex library inside your slurm cluster without any problems.</p>
</blockquote>
<h1 id="installing-nvidia-apex-library-inside-your-slurm-cluster--optional-">Installing Nvidia-Apex library inside your slurm cluster ( Optional )</h1>
<p>If you don't know about Apex library, you can visit Nvidia-Apex <a href="https://github.com/NVIDIA/apex">github page</a> to know more about Apex library. It is a set of utilites to help training model in mixed precision mode. It provide more numerical stable layer norm operations during training model in mixed precision mode, So that you can have both faster and stable training.</p>
<p>For <code>Nvidia-apex</code> to sucessfully compile, You need to upgrade your gcc --version to 7.3.0 .</p>
<pre><code class="hljs language-bash">> sudo yum install centos-release-scl
> sudo yum install devtoolset-7-gcc*
> scl <span class="hljs-built_in">enable</span> devtoolset-7 bash
> <span class="hljs-built_in">which</span> gcc
> gcc --version
(above bash script must output gcc version as 7.3.0)
</code></pre>
<p>Now We will compile and install Apex-library inside python3.8</p>
<pre><code class="hljs language-bash">
> git <span class="hljs-built_in">clone</span> https://github.com/NVIDIA/apex
> <span class="hljs-built_in">cd</span> apex
> <span class="hljs-built_in">export</span> TORCH_CUDA_ARCH_LIST=<span class="hljs-string">"6.0;7.0"</span>
> CUDA_HOME=<span class="hljs-variable">$HOME</span>/opt/cuda python3.8 -m pip install -v --no-cache-dir --global-option=<span class="hljs-string">"--cpp_ext"</span> --global-option=<span class="hljs-string">"--cuda_ext"</span> ./
</code></pre>
<p>Its takes around 4 - 6 minutes to compile and install apex.</p>
<h1 id="distributed-training-with-pytorch-and-slurm">Distributed Training with pytorch and slurm</h1>
<p>I will not give details on how to setup codebase in pytorch for distributed training, Its a huge topic and might need another blog post for it only. Meanwhile,  You can learn more about distributed training with pytorch <a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">here</a>. After you done setting up distributed training codebase in pytorch. You can start training using sbatch scripts.</p>
<h2 id="running-the-training-job-using-sbatch-script">Running the training Job Using SBATCH Script</h2>
<p>After we have done the cluster setup, preparing deep learning envs and building of the model, the last step is to finally run a job, i.e. to start the training. That is easily done by running sbatch script, which is basically a customized shell script. It effectively has two parts. The first part of the script is specific for the Slurm, it specifies the parameters for the Slurm job scheduler using the SBATCH command. The second part consists of bash (or some other shell) commands that you would normally run in terminal.</p>
<p>Below you will see demo SBATCH script (You need to modify sbatch script according to your needs).</p>
<blockquote>
<p>inside run_training.sh file.</p>
</blockquote>
<pre><code class="hljs language-bash"><span class="hljs-meta">#!/bin/sh</span>
 <span class="hljs-comment">#SBATCH --job-name=distributed_training</span>
  <span class="hljs-comment">#SBATCH --output=slurm_logs/slrm_stdout.%j</span>
  <span class="hljs-comment">#SBATCH --error=slurm_logs/slrm_stderr.%j</span>
  <span class="hljs-comment">#SBATCH --partition=gpu</span>
  <span class="hljs-comment">## make sure we don't clobber log files if jobs get restarted</span>
  <span class="hljs-comment">#SBATCH --open-mode=append</span>
  <span class="hljs-comment">#SBATCH --nodes=2</span>
  <span class="hljs-comment">#SBATCH --time=24:00:00</span>
  <span class="hljs-comment">## make sure we are told about preempts, and jobs running out of time, 60s beforehand</span>
  <span class="hljs-comment">#SBATCH --signal=USR1@60</span>
  <span class="hljs-comment">#SBATCH --cpus-per-task=5</span>
  <span class="hljs-comment">## srun forks ntasks_per_node times on each node</span>
  <span class="hljs-comment">#SBATCH --ntasks-per-node=1</span>
  <span class="hljs-comment">#SBATCH --mem=200G</span>
  <span class="hljs-comment">#SBATCH --gpus-per-node=1</span>

python3.8 train.py &#x3C;Your training args here>
</code></pre>
<p>Execute the sbatch script using the sbatch command line:</p>
<pre><code class="hljs language-bash">> sbatch run_training.sh
</code></pre>
<p>Running sbatch will return a Job ID for the scheduled job, for example:</p>
<pre><code>Submitted batch job 37
</code></pre>
<p>To keep track of the job’s state, run squeue and to keep track of the cluster’s state, run sinfo:</p>
<pre><code class="hljs language-bash">> squeue

JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)
3 gpu hostname &#x3C;username> CF 0:11 4 slurm-job-compute-0-[0-1]

> sinfo

PARTITION AVAIL TIMELIMIT NODES STATE NODELIST
gpu* up infinite 4 mix<span class="hljs-comment"># slurm-job-compute-0-[0-1]</span>
gpu* up infinite 6 idle~ slurm-job-compute-0-[2-7]
</code></pre>
<h1 id="summary">Summary</h1>
<p>In this blog post we learned how to setup slurm cluster in GCP  ( which is super easy ), setup own deeplearning environment from installing python, pytorch to compiling apex from source and finally run training using sbatch script and keeping track of job state using <code>sinfo</code>, <code>squeue</code> and <code>scontrol</code>.</p>
<h1 id="references">REFERENCES</h1>
<ol>
<li><a href="https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine">https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine</a></li>
<li><a href="https://cloud.google.com/deployment-manager/docs/deployments">https://cloud.google.com/deployment-manager/docs/deployments</a></li>
<li><a href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a></li>
<li><a href="https://github.com/NVIDIA/apex">https://github.com/NVIDIA/apex</a></li>
</ol></div></article><div class="mt-20"><form><textarea class="flex w-full max-h-40 p-3 rounded resize-y bg-gray-200 text-gray-900 placeholder-gray-500" rows="2" placeholder="Please login to leave a comment" disabled=""></textarea><div class="flex items-center mt-4"><button type="button" class="py-2 px-4 rounded bg-blue-600 text-white disabled:opacity-40 hover:bg-blue-700">Log In</button></div></form><div class="space-y-6 mt-10"></div></div></div></div></main></div><div class="static inset-x-0 bottom-0 content-center py-6 "><div class="flex flex-auto px-6 text-slate-400 gap-1.5 font-semibold "><div class="grow h-16 "> </div><div class="flex flex-col gap-1.5 items-center content-between "><div class="flex flex-auto items-center gap-2 "><svg aria-hidden="true" focusable="false" data-prefix="far" data-icon="copyright" class="svg-inline--fa fa-copyright " role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" style="font-size:20px"><path fill="currentColor" d="M256 0C114.6 0 0 114.6 0 256s114.6 256 256 256s256-114.6 256-256S397.4 0 256 0zM256 464c-114.7 0-208-93.31-208-208S141.3 48 256 48s208 93.31 208 208S370.7 464 256 464zM255.1 176C255.1 176 255.1 176 255.1 176c21.06 0 40.92 8.312 55.83 23.38c9.375 9.344 24.53 9.5 33.97 .1562c9.406-9.344 9.469-24.53 .1562-33.97c-24-24.22-55.95-37.56-89.95-37.56c0 0 .0313 0 0 0c-33.97 0-65.95 13.34-89.95 37.56c-49.44 49.88-49.44 131 0 180.9c24 24.22 55.98 37.56 89.95 37.56c.0313 0 0 0 0 0c34 0 65.95-13.34 89.95-37.56c9.312-9.438 9.25-24.62-.1562-33.97c-9.438-9.312-24.59-9.219-33.97 .1562c-14.91 15.06-34.77 23.38-55.83 23.38c0 0 .0313 0 0 0c-21.09 0-40.95-8.312-55.89-23.38c-30.94-31.22-30.94-82.03 0-113.3C214.2 184.3 234 176 255.1 176z"></path></svg><span>   2023  </span><span>Aaditya Chapagain</span></div><div class="ml-2 ">All Rights Reserved</div></div><div class="grow h-16 "> </div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"slug":"distributed-training-with-slurm-on-gcp","title":"Distributed Training of Deep Learning model with Slurm on GCP","date":"2020-09-15 12:00","modified":"2020-09-15 12:00","category":"Blog","summary":"Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python, slurm, distributed training, High performance computing, HPC, parallel processing, tensorflow, Pytorch, DL, Language model,","authors":["Aaditya Chapagain"],"status":"published","content":"\u003cp\u003eRecently, I was working on Big Machine Learning project. The task was to pretraining Large Machine learning models (with parameter in the range of several Billion ). And Normal training approch didn't work ( obviously ).With 8 GPU Volta core machines, it would take several months to complete just 1 epcoh of training, that's the point when i think of distributed training. I was using gcp ( google cloud ) for training models and found out that google already have support for High Performance Computing with Slurm . You can find Minimal working example on slurm from google codelabs here \u003ca href=\"https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\"\u003ehttps://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThrough this blog, I will try to explain what is HPC? , Why HPC ?, how can we train large Deep Learning models with slurm.\u003c/p\u003e\n\u003ch1 id=\"what-is-hpc-\"\u003eWhat is HPC ?\u003c/h1\u003e\n\u003cp\u003eHigh Performance Computing  (HPC) is the use of supercomputers and parallel processing techniques for solving complex mathematical and computational problems. HPC technology primarily focuses on developing parallel processing algorithms and systems by incorporating both administration and parallel computational techniques. HPC is typically used for solving advanced problems that require a lot time .\u003c/p\u003e\n\u003ch1 id=\"why-hpc-\"\u003eWhy HPC ?\u003c/h1\u003e\n\u003cp\u003eWhen you have loads of data and its processing takes really long time, then the approch \u003cstrong\u003edivide et impera\u003c/strong\u003e comes at hand.\u003c/p\u003e\n\u003cp\u003eWith HPC, we can divide any job so that every node processes different partitions of the data in parallel, speeding up the execution time.\u003c/p\u003e\n\u003ch1 id=\"slurm-workload-manager-on-gcp\"\u003eSlurm Workload Manager on GCP\u003c/h1\u003e\n\u003cp\u003eTo make computing with  slurm easier in GCP, Google and SchedMD ( Slurm's Creators ) joined forces and as a result, we can run a Slurm cluster on Google Cloud Platform. We don't have to worry about the parallel computing techniques since slurm takes care of that and GCP takes care of setting up a cluster and providing resources.\nBasic architectural diagram of a stand-alone Slurm Cluster in Google Cloud Platform.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ibb.co/SPhjRyM\"\u003e\u003cimg src=\"/images/slurm-archi.png\" alt=\"Slum Architecture in GCP\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAs we can see in above pictures, Slurm cluster contains three types of nodes: \u003cstrong\u003elogin\u003c/strong\u003e, \u003cstrong\u003econtroller\u003c/strong\u003e and \u003cstrong\u003eCompute\u003c/strong\u003e node.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLogin Node\u003c/strong\u003e serves as an interface for the user: user should communicate with the cluster exclusively through the login node (starting the job, requiring resources, ...)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eController Node\u003c/strong\u003e manages resources and job scheduling for the user.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompute Node\u003c/strong\u003e executes the job.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"setting-up-a-slurm-cluster-on-gcp\"\u003eSetting Up a Slurm Cluster on GCP\u003c/h1\u003e\n\u003cp\u003eBefore describing the setup, let us explain in short how does GCP implement a slurm cluster.\u003c/p\u003e\n\u003cp\u003eIn GCP, a cluster is realized as a \u003cstrong\u003edeployment\u003c/strong\u003e. A deployment is an instantiation of a set of resources  that are defined in a configuration. A deployment  can contain a number of resources, across a variety of Google Cloud Platform services. When you create a deployment, Deployment Manager creates all of the described resources in the respective Google Cloud Platform APIs.\u003c/p\u003e\n\u003cp\u003eThis brings us to the cluster's nodes. Each node in a cluster is actually a \u003cstrong\u003eVirtual Machine\u003c/strong\u003e.When a deployment is created, three new virtual machines appear in \"VM instances\" page, under \"Compute  Engine\". Those VMs are login instances, Controller instances, and compute image instance.\u003c/p\u003e\n\u003cp\u003eCompute Instance is a bit trickey part. One thing to notice is that deployment does not create compute instance,but exactly one compute image instance even if you request more compute nodes in your cluster. So, if a user requests 10 compute nodes for the cluster, those 10 virtual machines will not be immediately instantiated with the cluster deployment. Here's what is happening. These compute instances are created in the later step when you run a job and request the number of nodes for the job. Then the compute nodes will be allocated, and they will appear in the \"VM Instances\" page. Shortly after the job is completed, these virtual machines will be deallocated and will disappear from the list. This way a user gets new compute VMs everytime. The fact that deployment create compute image instance rather than compute nodes directly is that, you might not be using compute node all the time and creating compute nodes unnecessarily might affect your billing, so , slurm will create new compute nodes and use compute image instance as a templete to dynamically create new instance during running jobs, so that you will be billed for exact time period your compute node will run.\u003c/p\u003e\n\u003cp\u003eBelow you can see visual representation of the described process:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ibb.co/mFrGDy3\"\u003e\u003cimg src=\"/images/slurm-hpc.png\" alt=\"Slum Architecture in GCP\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFinally, let's head to the cluster setup. In this blog post, we will setup Slurm cluster for training Deep Learning Model with several nodes.Customize the information so that they will suit your needs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLaunch Google Cloud Shell\u003c/li\u003e\n\u003cli\u003eCheck that you already authenticated and that the project is already set to your \u003cstrong\u003ePROJECT_ID\u003c/strong\u003e:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e$ gcloud auth list\n\nCredentialed accounts:\n\u0026#x3C;your email\u003e\n\n$ gcloud config list project\n[core]\nproject = \u0026#x3C;PROJECT_ID\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col\u003e\n\u003cli\u003eClone git repository that contains the Slurm Google Cloud Platform deployment-manager files:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e$ git \u003cspan class=\"hljs-built_in\"\u003eclone\u003c/span\u003e https://github.com/SchedMD/slurm-gcp.git\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eSwitch to the Slurm deployment configuration directory:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e$ \u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e slurm-gcp\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eConfigure the Slurm Deployment YAML file. Provide information that suits your needs. There are plenty more parameters available, they can be found in SchedMD's GitHub repository. Below is the script that was sufficient for my needs.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-yaml\"\u003e\n\u003cspan class=\"hljs-comment\"\u003e# [START cluster_yaml]\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eimports:\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003epath:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eslurm.jinja\u003c/span\u003e\n\n\u003cspan class=\"hljs-attr\"\u003eresources:\u003c/span\u003e\n\u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eslurm-cluster\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003etype:\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eslurm.jinja\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003eproperties:\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003ecluster_name            :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eslurm-job\u003c/span\u003e\n\n    \u003cspan class=\"hljs-attr\"\u003ezone                    :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eus-central1-b\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003econtroller_machine_type :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003en1-standard-2\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003econtroller_disk_type      :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003epd-standard\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003econtroller_disk_size_gb   :\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eexternal_controller_ip    :\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n\n    \u003cspan class=\"hljs-attr\"\u003elogin_machine_type        :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003en1-standard-2\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003elogin_disk_type           :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003epd-standard\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003elogin_disk_size_gb        :\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e50\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eexternal_login_ips        :\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n\n    \u003cspan class=\"hljs-attr\"\u003ecompute_image_machine_type  :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003en1-standard-2\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003ecompute_image_disk_type   :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003epd-standard\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003ecompute_image_disk_size_gb:\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e\n    \u003cspan class=\"hljs-attr\"\u003eexternal_compute_ips      :\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\n    \u003cspan class=\"hljs-attr\"\u003epartitions :\u003c/span\u003e\n    \u003cspan class=\"hljs-bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ename           :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003egpu\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003emachine_type   :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003en1-standard-16\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003emax_node_count :\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ezone           :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003eus-central1-b\u003c/span\u003e\n        \u003cspan class=\"hljs-comment\"\u003e# cpu_platform           : Intel Skylake\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003epreemptible_bursting   :\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ecompute_disk_type      :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003epd-standard\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003ecompute_disk_size_gb   :\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003egpu_type               :\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003envidia-tesla-v100\u003c/span\u003e\n        \u003cspan class=\"hljs-attr\"\u003egpu_count              :\u003c/span\u003e \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e#  [END cluster_yaml]\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eIn the Cloud shell Session, execute the following command from the slurm-gcp folder:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e gcloud deployment-manager deployments create slurm-deployment  --config  slurm-cluster.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eThis command creates a deployment  named slurm-deployment. The operation can take few minutes to complete.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e5. Verify the deployment ( Navigation menu --\u003e  Deployment Manager)\n6. Verify the cluster's instances ( Navigation menu --\u003e Compute Engine --\u003e VM Instances)  There should be login , controller, and compute image instances. Compute image instance will live for small amount of time for setting up compute images, after that it will be off. and Compute Instances show up only when you allocate them for the sbatch job. they disappear shortly the job is completed.\u003c/p\u003e\n\u003col start=\"7\"\u003e\n\u003cli\u003e\n\u003cp\u003eLog in to login instance .\u003c/p\u003e\n\u003cp\u003eWhile logging in to the login instances if \u003cem\u003eSlurm is currently being installed/configured in the background.\u003c/em\u003e Don't install any packages  during that time, as that might disrupt the installation process of slurm. Wait for around 10 for slurm to be installed fully and you can log in to login instances and do whatever you want.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"setting-up-deep-learning-environment-inside-slurm-cluster\"\u003eSetting up Deep Learning Environment inside slurm cluster\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eSSH to login instance in your slurm cluster.\u003c/li\u003e\n\u003cli\u003eAfter sucessfully installing slurm on cluster,  ( /home , /apps ) directory of login instances will be shared with all other instances  like controller and compute instances. So that all deep learning  and python binaries must be installed inside \u003ccode\u003e/home\u003c/code\u003e or \u003ccode\u003e/apps\u003c/code\u003e directory.\u003c/li\u003e\n\u003cli\u003eInstalling python inside /home:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e ~\n\nsudo yum -y update; sudo yum groupinstall \u003cspan class=\"hljs-string\"\u003e\"Development Tools\"\u003c/span\u003e\n\nsudo yum -y install openssl-devel bzip2-devel libffi-devel\n\n\u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e tmp;\u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e tmp; wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz\n\ntar zxvf Python-3.8.3.tgz; \u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e Python-3.8.3\n\n./configure --enable-optimizations --prefix=\u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/opt/python-3.8\n\nmake\n\nmake altinstall\n\nvi ~/.bashrc\n(add line: \u003cspan class=\"hljs-built_in\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/opt/python-3.8/bin:\u003cspan class=\"hljs-variable\"\u003e$PATH\u003c/span\u003e) at the end of .bashrc \n\n\u003cspan class=\"hljs-built_in\"\u003esource\u003c/span\u003e ~/.bashrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e\u003e which python3.8\n(output should be: /home/\u0026#x3C;username\u003e/opt/python-3.8/bin/python3)\n\n# Now Lets install pytorch\n\n\u003e curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n\u003e  python3.8 get-pip.py\n\n\u003e python3.8 -m pip install torch==1.5.0\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eInstalling CUDA toolkits and Nvidia-drivers\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eDownload the latest Nvidia CUDA \u003ca href=\"https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/\"\u003erepository package\u003c/a\u003e cuda-repo-rhel7-*.rpm.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e \u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e ~\n\u003e wget https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-10.2.89-1.x86_64.rpm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eInstall the CUDA repository package. This will enable CUDA repository on your CentOS 7 Linux system:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e sudo rpm -i cuda-repo-*.rpm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eInstall cuda package from nvidia repository. Pytorch 1.5 works with cuda 10.2, so lets install cuda 10.2. Below command will install cuda and nvidia-drivers inside  \u003ccode\u003e/usr/local/cuda-10.2\u003c/code\u003e directory.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e sudo yum install cuda-10-2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eBy Now we know that only  \u003ccode\u003e/home\u003c/code\u003e and \u003ccode\u003e/apps\u003c/code\u003e directory were shared across all other instances. So lets copy cuda directory from \u003ccode\u003e/usr/local/\u003c/code\u003e to \u003ccode\u003e~/opt/cuda\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e ~/opt/cuda \n\u003e \u003cspan class=\"hljs-built_in\"\u003ecp\u003c/span\u003e -r /usr/local/cuda-10.2/* ~/opt/cuda/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eExport cuda path to Nvidia CUDA binary executables. Open the ~/.bashrc using your preferred text editor and add the following two lines\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-built_in\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/opt/cuda/bin:\u003cspan class=\"hljs-variable\"\u003e$PATH\u003c/span\u003e\n\u003cspan class=\"hljs-built_in\"\u003eexport\u003c/span\u003e LD_LIBRARY_PATH=\u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/opt/cuda/lib64:\u003cspan class=\"hljs-variable\"\u003e$LD_LIBRARY_PATH\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eNow Re-login or execute your updated ~/.bashrc file:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e \u003cspan class=\"hljs-built_in\"\u003esource\u003c/span\u003e ~/.bashrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eNow  Confirm the CUDA installation:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e nvcc --version\n\u003e nvidia-smi \u003cspan class=\"hljs-comment\"\u003e#( doesn't work as we dont have gpu in login instances )\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eBy Now your slurm system is ready and can run any training script using \u003ccode\u003esbatch\u003c/code\u003e scheduling. Below I will discuss on how to compile nvidia-apex library. It takes me 2 days just to make nvidia-apex work on slurm cluster.If you follow my above steps exaclty, I am sure you will be able to install nvidia apex library inside your slurm cluster without any problems.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"installing-nvidia-apex-library-inside-your-slurm-cluster--optional-\"\u003eInstalling Nvidia-Apex library inside your slurm cluster ( Optional )\u003c/h1\u003e\n\u003cp\u003eIf you don't know about Apex library, you can visit Nvidia-Apex \u003ca href=\"https://github.com/NVIDIA/apex\"\u003egithub page\u003c/a\u003e to know more about Apex library. It is a set of utilites to help training model in mixed precision mode. It provide more numerical stable layer norm operations during training model in mixed precision mode, So that you can have both faster and stable training.\u003c/p\u003e\n\u003cp\u003eFor \u003ccode\u003eNvidia-apex\u003c/code\u003e to sucessfully compile, You need to upgrade your gcc --version to 7.3.0 .\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e sudo yum install centos-release-scl\n\u003e sudo yum install devtoolset-7-gcc*\n\u003e scl \u003cspan class=\"hljs-built_in\"\u003eenable\u003c/span\u003e devtoolset-7 bash\n\u003e \u003cspan class=\"hljs-built_in\"\u003ewhich\u003c/span\u003e gcc\n\u003e gcc --version\n(above bash script must output gcc version as 7.3.0)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow We will compile and install Apex-library inside python3.8\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\n\u003e git \u003cspan class=\"hljs-built_in\"\u003eclone\u003c/span\u003e https://github.com/NVIDIA/apex\n\u003e \u003cspan class=\"hljs-built_in\"\u003ecd\u003c/span\u003e apex\n\u003e \u003cspan class=\"hljs-built_in\"\u003eexport\u003c/span\u003e TORCH_CUDA_ARCH_LIST=\u003cspan class=\"hljs-string\"\u003e\"6.0;7.0\"\u003c/span\u003e\n\u003e CUDA_HOME=\u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/opt/cuda python3.8 -m pip install -v --no-cache-dir --global-option=\u003cspan class=\"hljs-string\"\u003e\"--cpp_ext\"\u003c/span\u003e --global-option=\u003cspan class=\"hljs-string\"\u003e\"--cuda_ext\"\u003c/span\u003e ./\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIts takes around 4 - 6 minutes to compile and install apex.\u003c/p\u003e\n\u003ch1 id=\"distributed-training-with-pytorch-and-slurm\"\u003eDistributed Training with pytorch and slurm\u003c/h1\u003e\n\u003cp\u003eI will not give details on how to setup codebase in pytorch for distributed training, Its a huge topic and might need another blog post for it only. Meanwhile,  You can learn more about distributed training with pytorch \u003ca href=\"https://pytorch.org/tutorials/intermediate/dist_tuto.html\"\u003ehere\u003c/a\u003e. After you done setting up distributed training codebase in pytorch. You can start training using sbatch scripts.\u003c/p\u003e\n\u003ch2 id=\"running-the-training-job-using-sbatch-script\"\u003eRunning the training Job Using SBATCH Script\u003c/h2\u003e\n\u003cp\u003eAfter we have done the cluster setup, preparing deep learning envs and building of the model, the last step is to finally run a job, i.e. to start the training. That is easily done by running sbatch script, which is basically a customized shell script. It effectively has two parts. The first part of the script is specific for the Slurm, it specifies the parameters for the Slurm job scheduler using the SBATCH command. The second part consists of bash (or some other shell) commands that you would normally run in terminal.\u003c/p\u003e\n\u003cp\u003eBelow you will see demo SBATCH script (You need to modify sbatch script according to your needs).\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003einside run_training.sh file.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003cspan class=\"hljs-meta\"\u003e#!/bin/sh\u003c/span\u003e\n \u003cspan class=\"hljs-comment\"\u003e#SBATCH --job-name=distributed_training\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --output=slurm_logs/slrm_stdout.%j\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --error=slurm_logs/slrm_stderr.%j\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --partition=gpu\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e## make sure we don't clobber log files if jobs get restarted\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --open-mode=append\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --nodes=2\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --time=24:00:00\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e## make sure we are told about preempts, and jobs running out of time, 60s beforehand\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --signal=USR1@60\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --cpus-per-task=5\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e## srun forks ntasks_per_node times on each node\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --ntasks-per-node=1\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --mem=200G\u003c/span\u003e\n  \u003cspan class=\"hljs-comment\"\u003e#SBATCH --gpus-per-node=1\u003c/span\u003e\n\npython3.8 train.py \u0026#x3C;Your training args here\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecute the sbatch script using the sbatch command line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e sbatch run_training.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning sbatch will return a Job ID for the scheduled job, for example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSubmitted batch job 37\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo keep track of the job’s state, run squeue and to keep track of the cluster’s state, run sinfo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e\u003e squeue\n\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n3 gpu hostname \u0026#x3C;username\u003e CF 0:11 4 slurm-job-compute-0-[0-1]\n\n\u003e sinfo\n\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST\ngpu* up infinite 4 mix\u003cspan class=\"hljs-comment\"\u003e# slurm-job-compute-0-[0-1]\u003c/span\u003e\ngpu* up infinite 6 idle~ slurm-job-compute-0-[2-7]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"summary\"\u003eSummary\u003c/h1\u003e\n\u003cp\u003eIn this blog post we learned how to setup slurm cluster in GCP  ( which is super easy ), setup own deeplearning environment from installing python, pytorch to compiling apex from source and finally run training using sbatch script and keeping track of job state using \u003ccode\u003esinfo\u003c/code\u003e, \u003ccode\u003esqueue\u003c/code\u003e and \u003ccode\u003escontrol\u003c/code\u003e.\u003c/p\u003e\n\u003ch1 id=\"references\"\u003eREFERENCES\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\"\u003ehttps://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/deployment-manager/docs/deployments\"\u003ehttps://cloud.google.com/deployment-manager/docs/deployments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pytorch.org/tutorials/intermediate/dist_tuto.html\"\u003ehttps://pytorch.org/tutorials/intermediate/dist_tuto.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/NVIDIA/apex\"\u003ehttps://github.com/NVIDIA/apex\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"distributed-training-with-slurm-on-gcp"},"buildId":"0ig78gGTz-PJyHrhou7UP","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>