{"pageProps":{"allPosts":[{"slug":"rust_and_go_which_to_choose","title":"Which would I choose Rust or Go ? `Caught between a rock and a hard place`","date":"2023-03-30 10:15","modified":"2023-04-01 10:15","category":"Blog","summary":"Rust for developing systems and performance-critical applications, Go for implicity and scalability of mircro-services and distributed systems.","tags":"Machine Learning, Golang, Go, Rust, rustlang, programming languages, programming, tech, technology, technology stack, coding, learning rust, learning golang,","authors":["Aaditya Chapagain"],"status":"published","content":"\nRecently I had a talk with my friends about how they want to rewrite their entire company codebase with Rust, given their most of the codebases are bunch of mircro-services and few bit of AI models. Since I have had fair bit of participation on writing code on both Rust and Go. Here is my understanding of these languages and when to use these languages to get most out of it.\n\n## Table of contents\n\n## `Rust` and `Go` : Caught between a rock and and hard place are you ?\n\nIf I must say something, there is no shortage of languages to choose from, each with its own strengths and weaknesses.I think there are around 20 languages currently, that got fair share of market in current world, each can do lots of great things and great part of uniqueness in them. But given Rust and Go ( also known as Golang ),both are relatively new and popular programming languages that have been gaining lof of traction since their release to public world. I'll try to dive into the differences between Rust and Go and possibly we will figure out when to use these languages.\n\n## What is `Rust` mostly known for ?\n\n- **Memory Safety** : Talking about memory safety, Rust is at top of the chart and Its the best thing that rust offers to its users. Rust's `ownership` system ensures memory safety without the overhead of garbage collection, avoiding the likelihood of common programming errors like null pointer exception and buffer overflows.\n\n- **Concurrency** : Built-in support for concurrency with guaranteed Type Safety, enabling developers to write parallel code that is efficient, safe and easier to maintain without fear.\n\n- **Performance** : It compiles to machine code, which can be optimized for performance, making it ideal for systems programming and performance-critical aplication.\n\n- **C Interoperability** : Rust can easily with C, allowing developers to leverage libraries and gradually migrate legecy codebases.\n\n## What I don't like about `Rust` ?\n\n- **Steep Learning Curve** : Concepts like ownership, lifetimes, borrow and smart pointers takes significant time and effort to master, and can be challenging for newcomers to the field.\n\n- **Compilation Time** : Rust is compiled language. So, It needs to compile Before converting the code into working app and during compile time, Rust compiler try to do all the heavey lifting i.e. borrow checking, make sure app is type saftey, make sure there will be no memory leaks at the runtime and everything. Hence, for fairly complex and big application written in rust , it has ridiculous compilation time.\n\n## What is `Go` mostly known for ?\n\n- **Simplicity** : Go's syntax and design principles principles prioritize simplicity and readability, making it easy for developers to write and maintain code.\n\n- **Concurrency** : Go's built-in support for goroutines and channels simplifies concurrent programming, enabling developers to write scalable applications.\n\n- **Compilation speed** : Go compiles quickly, improving developer productivity and speeding up the development process.\n\n- **Strong Ecosystem** : Go has a growing ecosystem of libraries, tools, and frameworks that make it easy to develop web applications, microservices, and command-line utilities.\n\n## What I don't like about `Go` ?\n\n- **Garbage Collection** : Go's garbage collector can introduce latency and performance overhead in certain scenarios.\n\n- **Error Handling** : Go's error handling way is still in pre-mature phase and it makes code hard to understand.\n\n## Rust vs Go: Which is Right for your application ?\n\nWhen deciding between Rust and Go, consider the specific requirements of your project.\n\n### **When To use Rust**\n\n- You need a high level of control over system resources, with an emphasis on safety and concurrency\n- Systems programming, where performance and safety are critical\n- WebAssembly development, enabling high-performance web applications\n- Cryptography and security, where memory safety is cucial\n\n### **When to use Go**\n\n- Simplicity, readability, and scalability are your primary concerns\n- If you have a choice of converting your application code base to rust or Go ( Use Go instead )\n- Microservices, where its concurrency support enables scalable, performant applications.\n- Command-line utilites, where its simplicity and fast compilation times are beneficial\n- Networking and Distributed sytems, where its concurrency model simplifies complex interactions\n\n## What future holds for them\n\nBoth Rust and Go have bright futures in the tech sector, with each language carving out its own niche.\n\n### Rust's Future\n\nRust's focus on safety and performance makes it an attractive choice for critical infrastructure and systems programming. As more companies prioritize security and reliability, Rust is poised to gain adoption in various sectors, such as aerospace, finance, and healthcare. Additionally, Rust's support for WebAssembly will continue to drive its adoption in web development.\n\n### Go's Future\n\nGo's simplicity and strong support for concurrent programming make it well-suited for modern, scalable applications. As microservices and cloud-native development continue to grow in popularity, Go will likely see increased adoption in these areas. Additionally, Go's growing ecosystem and ongoing language improvements will continue to make it an attractive choice for a wide range of applications.\n\n## Finally\n\nIn summary, Rust and Go are both powerful languages with unique strengths and weaknesses. Rust is ideal for systems programming and performance-critical applications, while Go excels in simplicity and scalability for web applications, microservices, and distributed systems. By understanding the differences between these languages and the scenarios in which they excel, developers can make informed decisions about which language is best-suited for their needs.\n\n## REFERENCES\n\n- [Rust Ownership's](https://doc.rust-lang.org/stable/book/ch04-00-understanding-ownership.html)\n- [Why use Golang](https://www.uptech.team/blog/why-use-golang-for-your-project)\n- [Rust to webassembly](https://surma.dev/things/rust-to-webassembly/)\n- [Go for Begineers](https://www.freecodecamp.org/news/go-beginners-handbook/)\n\n## How To Get Started with Rust for Beginners ?\n\n- [Comprehensive Rust By google](https://github.com/google/comprehensive-rust)\n- [Official Rust Book](https://doc.rust-lang.org/stable/book/title-page.html)\n- [Official Rust Book with examples](https://doc.rust-lang.org/rust-by-example/)\n- [Official Rust repo for Rust learners](https://github.com/rust-lang/rustlings/)\n- [Awesome Rust learning Resources by Awesome People **Github Repo**](https://github.com/ctjhoa/rust-learning)\n"},{"slug":"useeffect-in-react-a-biginners-guide","title":"Understanding useEffect in React - A Beginner's Guide","date":"2023-03-27 19:45","modified":"2023-03-27 19:45","category":"Blog","summary":"UseEffect hook is a powerful and versatile tool in react that allows you to handle side effects in your components.","tags":"react, react hooks, useEffect, reactjs, react states, side effect, coding, js, nextjs","authors":["Aaditya Chapagain"],"status":"published","content":"\nReact is a popular library for building user interfaces, and one of its most powerful features is the ability to manage component state and side effects. Today, we'll explore the useEffect hook, which allows us to handle side effects in functional components. This guide will cover the basics of useEffect, provide code examples to demonstrate its use in various scenarios, and help you understand when and how to use it.\n\nOne of the most crucial ideas for knowing React today is how the 'useEffect' Hook functions. Consider that you have been using React for a while. In that instance, it is especially important to comprehend the differences between using useEffect and using the lifecycle methods of class-based components.\n\nIn the React Hooks period, it is crucial to comprehend how to call side effects from within functional components with useEffect. At first, dealing with the useEffect Hook's side effects may seem difficult, but gradually everything will make sense.\n\n## Table of contents\n\n## What are side effects?\n\nBefore we dive into how to use useEffect, let's discuss what side effects are in React.In simple terms, `side effects` is any action that occurs outside the scope of the component's `render` method. Such as\n\n1. Fetching data from API\n2. Setting up or cleaning event listeners\n3. updating the DOM directly\n4. Subscribing to a data stream\n\nThose side effects can cause issues in your application if they are not handled properly.For example, if you fetch data\nfrom an API inside a component every time it re-renders, you may end up making too many API calls, which can slow down your application and increase the cost of running it.\n\n## Using `useEffect`\n\nhe `useEffect` hook is part of React's [Hooks API](https://react.dev/reference/react) and used to manage side effects in functional component of react.It accepts two arguments:\n\n1. A function that represents the side effects to be performed\n2. An optional array of dependencies that determines when the effect should be executed.\n\n```js\nuseEffect(\n  () => {\n    // Side effect to be performed\n  },\n  [\n    /* dependencies */\n  ]\n);\n```\n\n### simulating `ComponentDidMount`\n\nConsider following example:\n\n```js\nimport { useState, useEffect } from \"react\";\n\nfunction App() {\n  const [data, setData] = useState(null);\n\n  useEffect(() => {\n    fetch(\"https://api.example.com/data\")\n      .then(response => response.json())\n      .then(data => setData(data));\n  }, []);\n\n  return (\n    <div>\n      {data ? (\n        <ul>\n          {data.map(item => (\n            <li key={item.id}>{item.name}</li>\n          ))}\n        </ul>\n      ) : (\n        <p>Loading data...</p>\n      )}\n    </div>\n  );\n}\n```\n\nAbove , Inside useEffect hook, we use the fetch function to make a GET request to an API. When the response comes back, we parse the JSON data and update the `data` state variable using the `setData` function.\n\nThe dependency array in `useEffect` is the array of values that the effect depends on which means if any of these values change, the effect will re-run or it will keep tracks of object in dependencies array and if any of the object in dependency array don't match with it previous version of data it will re-run the side effects written in `useEffect` hook.\n\nBut, In above case the dependency array is empty which means the effect will only run once in its lifetime ( when it is mounting the component ). In terms of class based react, it is exact way of setting states on `ComponentDidMount` lifecycle method.\n\n### Simulating `ComponentDidUpdate`\n\nConsider below example:\n\n```js\nimport { useState, useEffect } from \"react\";\n\nfunction App({ userID }) {\n  const [data, setData] = useState(null);\n\n  useEffect(() => {\n    fetch(`https://api.example.com/data?user_id=${userID}`)\n      .then(response => response.json())\n      .then(data => setData(data));\n  }, [userID]);\n\n  return (\n    <div>\n      {data ? (\n        <ul>\n          {data.map(item => (\n            <li key={item.id}>{item.name}</li>\n          ))}\n        </ul>\n      ) : (\n        <p>Loading data...</p>\n      )}\n    </div>\n  );\n}\n```\n\nIn above example, dependency array has one item which is also a props to the component. Hence, `useEffect` will track the changes on `userID` props, re-run the side effects present inside the useEffect when value of `userID` changes. Hence, It acts as a `componentDidUpdate` lifecycle method.\n\n### Simulating `componentWillUnmount`\n\nConsider below example:\n\n```js\nimport { useState, useEffect } from \"react\";\n\nfunction App() {\n  const [showComponent, setShowComponent] = useState(true);\n\n  useEffect(() => {\n    console.log(\"component mounted!\");\n\n    return () => {\n      console.log(\"Component unmounted\");\n    };\n  }, []);\n\n  return (\n    <div>\n      <button onClick={() => setShowComponent(!showComponent)}>\n        {\" \"}\n        Toggle Component\n      </button>\n      {showComponent && <ChildComponent />}\n    </div>\n  );\n}\n\nfunction ChildComponent() {\n  useEffect(() => {\n    console.log(\"Child component mounted\");\n    return () => {\n      console.log(\"Child component unmounted\");\n    };\n  }, []);\n\n  return (\n    <div>\n      <h2>Child Component</h2>\n    </div>\n  );\n}\n```\n\nIn this example, we define a state variable called 'showComponent' and a function called 'setShowComponent' that we use to toggle the visibility of a child component.\n\nWe use useEffect to log a message to the console when the component mounts and a different message when it unmounts. We also define a similar effect inside the child component.\n\nWhen we toggle the visibility of the child component using the button, we can see the messages in the console. When the child component is unmounted, the cleanup function is called which is similar functionality to the `componentWillUnmount` lifecycle method.\n\n## Conclusion\n\nthe useEffect hook is a powerful tool in React that allows you to handle side effects in your components. By using it correctly, you can make your code more efficient and easier to maintain.\n\nRemember, useEffect takes two arguments: a function and an array of dependencies. The function is the side effect you want to perform, and the dependencies are variables that the effect depends on.\n\nYou can use useEffect to fetch data from an API, update the title of the page, or set up timers and intervals. You can also use useEffect to return a cleanup function that cleans up any resources used by the effect.\n\nIt's important to pass the correct dependencies to useEffect to avoid unnecessary re-renders and memory leaks.\n\n## Tips and Tricks on using useEffect effectively:\n\n- **Use it for handling side effects**: useEffects is designed to handle side effects and you should only use it that way.If you need to modify the state of your component, you should use useState instead.\n\n- **Seperate concerns**: Use multiple useEffect hooks to separate unrelated side effects. This makes your code easier to understand and maintain.\n\n- **Optimize performance**: Be mindful of the dependencies you pass to useEffect. Provide an array of dependencies that accurately represent when the effect should be executed to avoid unnecessary updates.\n\n- **Avoid infinite loops**: When using a state variable as a dependency, make sure your effect does not cause an infinite loop by updating that state variable on every render.\n\n- **Don't forget the cleanup**: Always provide a cleanup function when your side effect requires it to avoid memory leaks and unexpected behavior.\n"},{"slug":"serverless-stacks-benefits","title":"Serverless stacks and Its benefits","date":"2023-03-05 14:00","modified":"2023-03-05 16:00","category":"Blog","summary":"Benefits of using serverless stacks.","tags":"serverless, aws, azure, serverless stacks, infrastructure","authors":["Aaditya Chapagain"],"status":"published","content":"\nBefore deep diving into the serverless stacks provided by AWS , we need to know what serverless really means.\n\n## What is serverless ?\n\nServerless is a cloud-native development model that allows developers to build and run applications without having to manage servers. The servers still gonna be involving while building serverless, but they are abstracted away from app development. A cloud provider like ( AWS, GCP & AZURE ) handles the routine work of provisioning, maintaining , and scaling the server infrastructure. Developers can simply package their code in containers for deployment.\n\nOnce deployed, serverless apps respond to demand and automatically scale up and downs as needed. Serverless architectures from cloud providers are usually metered on-demand through an event-driven execution model. As a result, **when a serverless function is sitting idle, it doesn’t cost anything**.\n\n## What are the services provided by aws serverless architecture ?\n\n[![serverless stack aws](/images/blog/serverlessstack.png)](https://aadiimages.imgix.net/images/blog/serverlessstack.png)\n\nIt provides many back end tasks like computing, databases, storage, processing and many more, this in result , allows the user to focus on his program and allows him to innovate.\n\n- Compute using lambda\n\n- Storage using Amazon S3\n\n- Database using Amazon Dynamodb\n\n- API proxy using amazon API Gateway\n\n- Application Integration using Amazon SNS\n\n- Orchestration using Step Functions\n\n- Analytics using Amazon Kinesis\n\n- Security & Access Control using its identity and Access Management\n\n## Pros of AWS Serverless Computing\n\n- Servers need no attention for installation and maintenance.\n\n- Payment is as per the throughput, making it value for money.\n\n- You can choose appropriate settings as per your products requirements, thus paying only for the functions you use.\n\n- Serverless have in-built availability and fault tolerance. User need not architect for these capabilities since the services running the application provide them by default.\n\n- Write code and deploy with just few steps, and it will be available to the world within few minutes. Thus no need to put any effort into or for creating and managing servers.\n\n## Cons of AWS Serverless Computing\n\n- [Serverless Architecture](https://aws.amazon.com/lambda/serverless-architectures-learn-more/) excecutes commands and functions on temporarily created container. So if a client performs few tasks on your app, the serverless architecture will create a temporary container and will destroy it as soon as the client is finished performing tasks, this results in delays which are also known as cold start.\n\n- As serverless architecture is based on the temporarily created containers, the usable memory is thus limited hindering the processes that require a lot of processing.\n\n- Another issue with Lambda is that it decides which third-party apps can be used to work on it, thus giving up a lot of control over your application.\n\n- Monitoring and Debugging are quite restrictive to what the vendor provides. It is fundamental with Lambda too. It lacks proper operational tools for monitoring and debugging.\n\n- Lack of local testing options.\n"},{"slug":"useful-python-decorators-to-take-your-code-to-moon-and-beyond","title":"Useful Python decorator to take your code to MARS and beyond","date":"2022-04-15 12:00","modified":"2022-04-15 12:00","category":"Blog","summary":"Do more thing with less code without compromising on quality","tags":"Machine Learning, python, coding, decorator, callbacks, developer, python skill,","authors":["Aaditya Chapagain"],"status":"published","content":"\nIn this blog post, we will explore 10 helpful decorators which I regularly use in my projects to extend my mode with extra functionalites. We'll dive into each decorator, look at the code and play with some hands-on examples. If you are also Python developer, these decorators will help you massively in your python projects.\n\n## Table of contents\n\n## @logger\n\nIf you are unfamiliar with decorators, consider reading my previous [blog post](https://aadityachapagain.com/posts/understanding-python-decorator-with-its-usage).\n\nlogger is a function that takes a function as input and return a function as output.The output function is usually an extended version of the input. In our case, we want the output function to surround the call of the input function with `start` and `end` statements.\n\nHere's the simple implementation of the `logger` decorator.\n\n```python\nfrom functools import wraps\n\ndef logger(func):\n  @wraps(func)\n  def wrapper(*args, **kwargs):\n    print(f\"----- {function.__name__}: start -----\")\n    output = func(*args, **kwargs)\n    print(f\"----- {function.__name__}: end -----\")\n    return output\n  return wrapper\n\n\n# now you can apply logger to any other functions\n@logger\ndef some_func(text):\n  # do something here\n\n```\n\n## @lru_cache\n\nThis is a built-in decorator that you can import from `functools`.\n\nIt caches the return values of a function , using a least-recently-used( LRU ) algorithm to discard the least-used values when the cache is full.\n\nI typically use this decorator for long-running tasks that don't change the output with the same input like querying a database, requesting a static remote web page, or running some heavy processing.\n\n```python\n\nimport random\nimport time\nfrom functools import lru_cache\n\n\n@lru_cache(maxsize=None)\ndef heavy_processing(n):\n    sleep_time = n + random.random()\n    time.sleep(sleep_time)\n\n# first time\n%%time\nheavy_processing(0)\n# CPU times: user 363 µs, sys: 727 µs, total: 1.09 ms\n# Wall time: 694 ms\n\n# second time\n%%time\nheavy_processing(0)\n# CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n# Wall time: 8.11 µs\n\n# third time\n%%time\nheavy_processing(0)\n# CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n# Wall time: 7.15 µs\n\n```\n\nif you want to implement a cache decorator yourself from scratch, here's how you'd do it:\n\n- You add an empty dictionary as an attribute to the wrapper function to store previously computed values by the input function\n- When calling the input function, you first check if its arguments are present in the cache. If it's the case, return the result. Otherwise, compute it and put it in the cache.\n\n```python\n\nfrom functools import wraps\n\ndef cache(function):\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        cache_key = args + tuple(kwargs.items())\n        if cache_key in wrapper.cache:\n            output = wrapper.cache[cache_key]\n        else:\n            output = function(*args)\n            wrapper.cache[cache_key] = output\n        return output\n    wrapper.cache = dict()\n    return wrapper\n\n@cache\ndef heavy_processing(n):\n    sleep_time = n + random.random()\n    time.sleep(sleep_time)\n\n\n%%time\nheavy_processing(1)\n# CPU times: user 446 µs, sys: 864 µs, total: 1.31 ms\n# Wall time: 1.06 s\n\n%%time\nheavy_processing(1)\n# CPU times: user 11 µs, sys: 0 ns, total: 11 µs\n# Wall time: 13.1 µs\n\n```\n\n## @repeat\n\nThis decorator causes a function to be called multiple times in a row.\n\nThis can be useful for debugging purposes, stress tests, or automating the repetition of multiple tasks.\n\nUnlike the previous decorators, this one expects an input parameter.\n\n```python\n\ndef repeat(number_of_times):\n    def decorate(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for _ in range(number_of_times):\n                func(*args, **kwargs)\n        return wrapper\n    return decorate\n\n\n@repeat(5)\ndef dummy():\n    print(\"hello\")\n\ndummy()\n# hello\n# hello\n# hello\n# hello\n# hello\n\n```\n\n## @timeit\n\nThis decorator measures the execution time of a function and prints the result: this serves as debugging or monitoring.\n\nIn the following snippet, the `timeit` decorator measures the time it takes for the `process_data` function to execute and prints out the elapsed time in seconds.\n\n```python\nimport time\nfrom functools import wraps\n\ndef timeit(func):\n  @wraps(func):\n  def wrapper(*args, **kwargs):\n    start = time.perf_counter()\n    result = func(*args, **kwargs)\n    end = time.perf_counter()\n    print(f'{func.__name__} took {end - start:.6f} seconds to complete')\n    return result\n  return wrapper\n\n@timeit\ndef process_data():\n  time.sleep(1)\n\nprocess_data()\n\n```\n\n## @retry\n\nThis decorator forces a function to retry a number of times when it encounters an exception.\n\nIt takes three arguments: the number of retries, the exception to catch and retry on, and the sleep time between retries.\n\n```python\n\nimport random\nimport time\nfrom functools import wraps\n\ndef retry(num_retries, exception_to_check, sleep_time=0):\n    \"\"\"\n    Decorator that retries the execution of a function if it raises a specific exception.\n    \"\"\"\n    def decorate(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for i in range(1, num_retries+1):\n                try:\n                    return func(*args, **kwargs)\n                except exception_to_check as e:\n                    print(f\"{func.__name__} raised {e.__class__.__name__}. Retrying...\")\n                    if i < num_retries:\n                        time.sleep(sleep_time)\n            # Raise the exception if the function was not successful after the specified number of retries\n            raise e\n        return wrapper\n    return decorate\n\n@retry(num_retries=3, exception_to_check=ValueError, sleep_time=1)\ndef random_value():\n    value = random.randint(1, 5)\n    if value == 3:\n        raise ValueError(\"Value cannot be 3\")\n    return value\n\nrandom_value()\n# random_value raised ValueError. Retrying...\n# 1\n\nrandom_value()\n# 5\n\n```\n\n## @countcall\n\nThis decorator counts the number of times a function has been called.\n\nThis number is stored in the wrapper attribute `count`.\n\n```python\n\nfrom functools import wraps\n\ndef countcall(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        wrapper.count += 1\n        result = func(*args, **kwargs)\n        print(f'{func.__name__} has been called {wrapper.count} times')\n        return result\n    wrapper.count = 0\n    return wrapper\n\n@countcall\ndef process_data():\n    pass\n\nprocess_data()\nprocess_data has been called 1 times\nprocess_data()\nprocess_data has been called 2 times\nprocess_data()\nprocess_data has been called 3 times\n```\n\n## @rate_limited\n\nThis is a decorator that limits the rate at which a function can be called, by sleeping an amount of time if the function is called too frequently.\n\n```python\n\nimport time\nfrom functools import wraps\n\ndef rate_limited(max_per_second):\n    min_interval = 1.0 / float(max_per_second)\n    def decorate(func):\n        last_time_called = 0.0\n        @wraps(func)\n        def rate_limited_function(*args, **kargs):\n            elapsed = time.perf_counter() - last_time_called\n            left_to_wait = min_interval - elapsed\n            if left_to_wait > 0:\n                time.sleep(left_to_wait)\n            ret = func(*args, **kargs)\n            last_time_called = time.perf_counter()\n            return ret\n        return rate_limited_function\n    return decorate\n\n```\n\n> This function hence introduces a slight time overhead between the calls but ensures that the rate limit is not exceeded.\n\nThere's also a third-party package that implements API rate limit: it's called **ratelimit**.\n\n```bash\npip install ratelimit\n```\n\nTo use this package, simply decorate any function that makes an API call:\n\n```python\n\nfrom ratelimit import limits\n\nimport requests\n\nFIFTEEN_MINUTES = 900\n\n@limits(calls=15, period=FIFTEEN_MINUTES)\ndef call_api(url):\n  response = requests.get(url)\n\n  if response.status_code != 200:\n    raise Execption('API response: {}'.format(response.status_code))\n  return response\n\n```\n\nif the decorated function is called more times than allowed a `ratelimit.RateLimitException` is raised.\n\nTo be able to handle this exception, you can use the `sleep_and_retry` decorator in combination with the `ratelimit` decorator.\n\n```python\n\n@sleep_and_retry\n@limits(calls=15, period=FIFTEEN_MINUTES)\ndef call_api(url):\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise Exception('API response: {}'.format(response.status_code))\n    return response\n```\n\nThis causes the function to sleep the remaining amount of time before being executed again.\n\n## @register\n\nIf your pyton script accidentally terminates and you still want to perform some tasks to save your work, perform some tasks to save your work, perform cleanup or print a message, I find that the register decorator is quite handy in this context.\n\n```python\n\nfrom atexit import register\n\n@register\ndef terminate():\n    perform_some_cleanup()\n    print(\"Goodbye!\")\n\nwhile True:\n    print(\"Hello\")\n\n```\n\n## @dataclass\n\nThe `@dataclass` decorator in Python is used to decorate classes.\n\nIt automatically generates special methods such as `__init__`, `__repr__`,\n`__eq__`, `__lt__`, and `__str__` for classes that primarily store data. This can reduce the boilerplate code and make the classes more readable and maintainable.\n\nIt also provides nifty methods off-the-shelf to represent objects nicely, convert them into JSON format, make them immutable, etc.\n\nThe `@dataclass` decorator was introduced in Python 3.7 and is available in the standard library.\n\n```python\n\nfrom dataclasses import dataclass,\n\n@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    age: int\n    job: str\n\n    def __eq__(self, other):\n        if isinstance(other, Person):\n            return self.age == other.age\n        return NotImplemented\n\n    def __lt__(self, other):\n        if isinstance(other, Person):\n            return self.age < other.age\n        return NotImplemented\n\n\njohn = Person(first_name=\"John\",\n              last_name=\"Doe\",\n              age=30,\n              job=\"doctor\",)\n\nanne = Person(first_name=\"Anne\",\n              last_name=\"Smith\",\n              age=40,\n              job=\"software engineer\",)\n\nprint(john == anne)\n# False\n\nprint(anne > john)\n# True\n\nasdict(anne)\n#{'first_name': 'Anne',\n# 'last_name': 'Smith',\n# 'age': 40,\n# 'job': 'software engineer'}\n\n```\n\n## @singledispatch\n\nThis decorator allows a function to have different implementations for different types of arguments.\n\n```python\n\nfrom functools import singledispatch\n\n@singledispatch\ndef fun(arg):\n    print(\"Called with a single argument\")\n\n@fun.register(int)\ndef _(arg):\n    print(\"Called with an integer\")\n\n@fun.register(list)\ndef _(arg):\n    print(\"Called with a list\")\n\nfun(1)  # Prints \"Called with an integer\"\nfun([1, 2, 3])  # Prints \"Called with a list\"\n\n```\n\n## Conclusion\n\nDecorators are useful abstractions to extend your code with extra functionalities like caching, automatic retry, rate limiting, logging, or turning your classes into supercharged data containers.\n\nHere's a [list](https://github.com/lord63/awesome-python-decorator) of awesome decorators to get inspired.\n\nThanks for reading!.\n"},{"slug":"understanding-python-decorator-with-its-usage","title":"Understanding Python Decorator and Its Usage","date":"2022-03-15 12:00","modified":"2022-03-15 12:00","category":"Blog","summary":"Upskill your python using awesome decorators","tags":"Machine Learning, python, coding, decorator, callbacks, developer, python skill,","authors":["Aaditya Chapagain"],"status":"published","content":"\nYou might have seen python code that has `@<function_or_class>` written immediately above a function or Class.\nFor Example, `@staticmethod` right above static method or `@classmethod` right above class method. Those are\nactually Python decorators. Decorators allow an existing function to be extended without changing the source code.\n\nIn this blog, we will comprehend a decorator structure, examine advanced behaviours including drawbacks and ways to overcome it, nested decorators, stacking decorators and eventually some practical applications, develop our own multi-purpose decorators.\n\n## Table of contents\n\n## Writing Decorator\n\nDecorators are often referred to as function wrappers because they accept a function as a paramter and return a modified version of the function that has additional features or capabilites.\n\n```python\n\ndef simple_decorator(func):\n  def wrapper(*args, **kwargs):\n    # do something before function execution\n    result = func (*args, **kwargs)\n    # do something after function execution\n    return result\n  return wrapper\n\n```\n\nwe can see from the construct above that line 4 is where the function really executes, but we can change what happens before to, during and even after a function execution. Althouh decorators may alter a function's input, output, or behaviour, it is preferable to build them in a way that does not make the function they are wrapping less understandable.\n\n> **Decorators are best used to add common behaviour to multiple functions without modifying every function manually**\n\n## Advanced behaviours of Decorators\n\n### Retain metadata of wrapped function\n\nOne major **drawback** of using decorators is that the metadata of function will be destroyed by decorator when actually calling that function in code, because we are returning a wrapper function in place of the original function in the code sample from the previous section, all decorated functions will have their `__name__` and `signature` information changed to wrapper's.\n\n```python\n\n@simple_decorator\ndef func_add(a, b):\n  return a + b\n\nprint(func_add.__name__)\n# wrapper\n\n```\n\nTechnically, this wouldn't impact how the fucntion or decorator was meant to be used, but it's still advisable to avoid any unexpected outcomes when using a decorator. This can be accomplished quickly by decorating the wrapper function with the `@wraps` decorator, as illustrated below.The decorator can still be used in the same way, but now the wrapped function's metadata won't be changed.\n\n```python\n\nfrom functools import wraps\n\ndef sample_decorator(func):\n  @wraps(func)\n  def wrapper(*args, **kwargs):\n    # do something before function exec\n    result = func(*args, **kwargs)\n    # do something after function exec\n    return result\n  return wrapper\n\n```\n\n### Decorators that accept arguments\n\nUnder the hood we all know that decorators are just a function wrapper. So, it is possible for decorator to receive arguments to make make this happen , we just need to slight change in coding our decorator.\nWe will get decorator that will take arguments, If only we just wrap the existing decorator with another function with arguments we wanna pass to the decorator. Easy right !.\n\nExample below I have a `debug_decorator` that takes parameters and returns a `decorator` decorator that wraps thea original function in a new one. The several nested function in this can make it appear fairly confusing at first but first creating the original decorator before wrapping it to take argumeants will make it easy to code and understand.\n\n```python\n\nfrom functools import wraps\n\ndebug_mode = True\n\n\ndef debug_decorator(debug_mode):\n    \"\"\"Example: Passing arguments to a decorator\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if debug_mode:\n                print(f\"Function called: {func.__name__}\")\n            result = func(*args, **kwargs)\n            return result\n        return wrapper\n    return decorator\n\n\n@debug_decorator(debug_mode)\ndef func_add(a, b):\n    return a + b\n\n\nfunc_add(1, 2)\n# Function called: func_add\n# 3\n```\n\n### Stacking decorators\n\nDecorators, as previously indicated, permit the extension of current functionalities. To add extra extensions, it is possible to stack several decorators over a function. The decorator will be piled in the same order as tahe order of execution.\n\nOne thing to remember is that if time-sensitive decorators are layered, they should be added at last. So that they can accurately reflect the execution of time without being influeced by other decorators. Decorators that measure the execution of time of a function, for instance, should be the last to be executed.\n\nNow, that we are familiar with a decorator's structure and sophisticated behaviours, we can explore their actual use!\n\n## Usage of Decorators\n\n### Measuring execution time of functions\n\nThe `timer` decorator can measure the execution time of the wrapped function by recording the start time and end time of the function execution and printing the results to the console.\n\nIn the code snippet below, we measure the `start_time` and `end_time` before and after function execution.\n\n```python\nimport time\n\nfrom functools import wraps\n\ndef timer(func):\n    \"\"\"Example: Measure execution time of function\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        print(f\"Execution time: {round(end_time - start_time, 4)}\")\n        return result\n    return wrapper\n\n\n@timer\ndef func_add(a, b):\n    time.sleep(2)\n    return a + b\n\n\nfunc_add(1, 2)\n# Execution time: 2.0064\n\n```\n\n### Debug with logging\n\nThe `logging` decorator can be used to log information to a console or log file and is useful for debugging. Below we'll use the `logging` python package to perform logging.\n\n```python\n\nimport logging\n\nfrom datetime import datetime\nfrom functools import wraps\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef logging(func):\n    \"\"\"Example: Logging with decorator\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        log_time = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n        logger.info(f\"{log_time}: {func.__name__} called\")\n        result = func(*args, **kwargs)\n        return result\n    return wrapper\n\n```\n\n### Registering Plugins\n\nDecorators don’t have to wrap the function they’re decorating. They can also simply register that a function exists and return it unwrapped. This can be used, for instance, to create a light-weight plug-in architecture:\n\n```python\nimport random\nPLUGINS = dict()\n\ndef register(func):\n    \"\"\"Register a function as a plug-in\"\"\"\n    PLUGINS[func.__name__] = func\n    return func\n\n@register\ndef say_hello(name):\n    return f\"Hello {name}\"\n\n@register\ndef be_awesome(name):\n    return f\"Yo {name}, together we are the awesomest!\"\n\ndef randomly_greet(name):\n    greeter, greeter_func = random.choice(list(PLUGINS.items()))\n    print(f\"Using {greeter!r}\")\n    return greeter_func(name)\n\n```\n\nThe `@register` decorator simply stores a reference to the decorated function in the global `PLUGINS` dict. Note that you do not have to write an inner function or use @functools.wraps in this example because you are returning the original function unmodified.\n\nThe randomly_greet() function randomly chooses one of the registered functions to use. Note that the PLUGINS dictionary already contains references to each function object that is registered as a plugin:\n\n```python\n>>> PLUGINS\n{'say_hello': <function say_hello at 0x7f768eae6730>,\n 'be_awesome': <function be_awesome at 0x7f768eae67b8>}\n\n>>> randomly_greet(\"Alice\")\nUsing 'say_hello'\n'Hello Alice'\n\n```\n\nThe main benefit of this simple plugin architecture is that you do not need to maintain a list of which plugins exist. That list is created when the plugins register themselves. This makes it trivial to add a new plugin: just define the function and decorate it with `@register`.\n\nIf you are familiar with `globals()` in Python, you might see some similarities to how the plugin architecture works. `globals()` gives access to all global variables in the current scope, including your plugins:\n\n```python\n\n>>> globals()\n{..., # Lots of variables not shown here.\n 'say_hello': <function say_hello at 0x7f768eae6730>,\n 'be_awesome': <function be_awesome at 0x7f768eae67b8>,\n 'randomly_greet': <function randomly_greet at 0x7f768eae6840>}\n\n```\n\nUsing the `@register` decorator, you can create your own curated list of interesting variables, effectively hand-picking some functions from `globals()`.\n\n### Creating a singleton class\n\n> **Singleton class is a design pattern that restricts the instantiation of a class and ensures that only one instance of the class exists.**\n\nSingleton class is helpful when there is a cap on the number of concurrent users who can use a shared resource or when there is only one point of access to a resource. In python singleton class can be created just using decorator.\n\nSingle instantiation can be guaranteed by specifically coding singleton classes. But, utilising decorators is a clever method to reuse the code for several classes if there are several singleton classes.\n\n```python\nfrom functools import wraps\n\n\ndef singleton(cls):\n    \"\"\"Example: Create singleton class with decorator\"\"\"\n    instances = {}\n\n    @wraps(cls)\n    def wrapper(*args, **kwargs):\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n    return wrapper\n\n\n@singleton\nclass SampleClass:\n    def __init__(self):\n        pass\n\n\nsingleton_class = SampleClass()\nsingleton_class2 = SampleClass()\nprint(singleton_class == singleton_class2)\n# True\n\n```\n\nHope, you now have a solid understanding of decorators fundamentals, practical advice and real-world examples. Other applications include the use of decorator for caching, memory management, and timeout precedures. These decorators are more sophisticated, thus using built-in decorators from python or decorators from 3^<sup>rd</sup> party python packages is preferable to creating them from scratch.\n\nHope In future , I will come up with new blog posts explaining all the really really complicated python decorators.\n\n## REFERENCES\n\n- [https://www.wikiwand.com/en/Singleton_pattern](https://www.wikiwand.com/en/Singleton_pattern)\n\n- [https://peps.python.org/pep-0318/](https://peps.python.org/pep-0318/)\n\n- [https://realpython.com/primer-on-python-decorators/](https://realpython.com/primer-on-python-decorators/)\n"},{"slug":"distributed-training-with-slurm-on-gcp","title":"Distributed Training of Deep Learning model with Slurm on GCP","date":"2020-09-15 12:00","modified":"2020-09-15 12:00","category":"Blog","summary":"Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python, slurm, distributed training, High performance computing, HPC, parallel processing, tensorflow, Pytorch, DL, Language model,","authors":["Aaditya Chapagain"],"status":"published","content":"\nRecently, I was working on Big Machine Learning project. The task was to pretraining Large Machine learning models (with parameter in the range of several Billion ). And Normal training approch didn't work ( obviously ).With 8 GPU Volta core machines, it would take several months to complete just 1 epcoh of training, that's the point when i think of distributed training. I was using gcp ( google cloud ) for training models and found out that google already have support for High Performance Computing with Slurm . You can find Minimal working example on slurm from google codelabs here [https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine](https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine).\n\nThrough this blog, I will try to explain what is HPC? , Why HPC ?, how can we train large Deep Learning models with slurm.\n\n# What is HPC ?\n\nHigh Performance Computing (HPC) is the use of supercomputers and parallel processing techniques for solving complex mathematical and computational problems. HPC technology primarily focuses on developing parallel processing algorithms and systems by incorporating both administration and parallel computational techniques. HPC is typically used for solving advanced problems that require a lot time .\n\n# Why HPC ?\n\nWhen you have loads of data and its processing takes really long time, then the approch **divide et impera** comes at hand.\n\nWith HPC, we can divide any job so that every node processes different partitions of the data in parallel, speeding up the execution time.\n\n# Slurm Workload Manager on GCP\n\nTo make computing with slurm easier in GCP, Google and SchedMD ( Slurm's Creators ) joined forces and as a result, we can run a Slurm cluster on Google Cloud Platform. We don't have to worry about the parallel computing techniques since slurm takes care of that and GCP takes care of setting up a cluster and providing resources.\nBasic architectural diagram of a stand-alone Slurm Cluster in Google Cloud Platform.\n\n[![Slum Architecture in GCP](/images/slurm-archi.png)](https://ibb.co/SPhjRyM)\n\nAs we can see in above pictures, Slurm cluster contains three types of nodes: **login**, **controller** and **Compute** node.\n\n- **Login Node** serves as an interface for the user: user should communicate with the cluster exclusively through the login node (starting the job, requiring resources, ...)\n- **Controller Node** manages resources and job scheduling for the user.\n- **Compute Node** executes the job.\n\n# Setting Up a Slurm Cluster on GCP\n\nBefore describing the setup, let us explain in short how does GCP implement a slurm cluster.\n\nIn GCP, a cluster is realized as a **deployment**. A deployment is an instantiation of a set of resources that are defined in a configuration. A deployment can contain a number of resources, across a variety of Google Cloud Platform services. When you create a deployment, Deployment Manager creates all of the described resources in the respective Google Cloud Platform APIs.\n\nThis brings us to the cluster's nodes. Each node in a cluster is actually a **Virtual Machine**.When a deployment is created, three new virtual machines appear in \"VM instances\" page, under \"Compute Engine\". Those VMs are login instances, Controller instances, and compute image instance.\n\nCompute Instance is a bit trickey part. One thing to notice is that deployment does not create compute instance,but exactly one compute image instance even if you request more compute nodes in your cluster. So, if a user requests 10 compute nodes for the cluster, those 10 virtual machines will not be immediately instantiated with the cluster deployment. Here's what is happening. These compute instances are created in the later step when you run a job and request the number of nodes for the job. Then the compute nodes will be allocated, and they will appear in the \"VM Instances\" page. Shortly after the job is completed, these virtual machines will be deallocated and will disappear from the list. This way a user gets new compute VMs everytime. The fact that deployment create compute image instance rather than compute nodes directly is that, you might not be using compute node all the time and creating compute nodes unnecessarily might affect your billing, so , slurm will create new compute nodes and use compute image instance as a templete to dynamically create new instance during running jobs, so that you will be billed for exact time period your compute node will run.\n\nBelow you can see visual representation of the described process:\n\n[![Slum Architecture in GCP](/images/slurm-hpc.png)](https://ibb.co/mFrGDy3)\n\nFinally, let's head to the cluster setup. In this blog post, we will setup Slurm cluster for training Deep Learning Model with several nodes.Customize the information so that they will suit your needs:\n\n1. Launch Google Cloud Shell\n2. Check that you already authenticated and that the project is already set to your **PROJECT_ID**:\n\n```bash\n$ gcloud auth list\n\nCredentialed accounts:\n<your email>\n\n$ gcloud config list project\n[core]\nproject = <PROJECT_ID>\n\n```\n\n1. Clone git repository that contains the Slurm Google Cloud Platform deployment-manager files:\n\n```bash\n$ git clone https://github.com/SchedMD/slurm-gcp.git\n\n```\n\n2. Switch to the Slurm deployment configuration directory:\n\n```bash\n$ cd slurm-gcp\n```\n\n3. Configure the Slurm Deployment YAML file. Provide information that suits your needs. There are plenty more parameters available, they can be found in SchedMD's GitHub repository. Below is the script that was sufficient for my needs.\n\n```yaml\n\n# [START cluster_yaml]\nimports:\n- path: slurm.jinja\n\nresources:\n- name: slurm-cluster\ntype: slurm.jinja\nproperties:\n    cluster_name            : slurm-job\n\n    zone                    : us-central1-b\n    controller_machine_type : n1-standard-2\n    controller_disk_type      : pd-standard\n    controller_disk_size_gb   : 50\n    external_controller_ip    : True\n\n    login_machine_type        : n1-standard-2\n    login_disk_type           : pd-standard\n    login_disk_size_gb        : 50\n    external_login_ips        : True\n\n    compute_image_machine_type  : n1-standard-2\n    compute_image_disk_type   : pd-standard\n    compute_image_disk_size_gb: 200\n    external_compute_ips      : False\n\n    partitions :\n    - name           : gpu\n        machine_type   : n1-standard-16\n        max_node_count : 10\n        zone           : us-central1-b\n        # cpu_platform           : Intel Skylake\n        preemptible_bursting   : True\n        compute_disk_type      : pd-standard\n        compute_disk_size_gb   : 200\n        gpu_type               : nvidia-tesla-v100\n        gpu_count              : 1\n\n#  [END cluster_yaml]\n\n```\n\n4. In the Cloud shell Session, execute the following command from the slurm-gcp folder:\n\n```bash\n> gcloud deployment-manager deployments create slurm-deployment  --config  slurm-cluster.yaml\n```\n\n    This command creates a deployment  named slurm-deployment. The operation can take few minutes to complete.\n\n5. Verify the deployment ( Navigation menu --> Deployment Manager)\n6. Verify the cluster's instances ( Navigation menu --> Compute Engine --> VM Instances) There should be login , controller, and compute image instances. Compute image instance will live for small amount of time for setting up compute images, after that it will be off. and Compute Instances show up only when you allocate them for the sbatch job. they disappear shortly the job is completed.\n7. Log in to login instance .\n\n   While logging in to the login instances if _Slurm is currently being installed/configured in the background._ Don't install any packages during that time, as that might disrupt the installation process of slurm. Wait for around 10 for slurm to be installed fully and you can log in to login instances and do whatever you want.\n\n# Setting up Deep Learning Environment inside slurm cluster\n\n1. SSH to login instance in your slurm cluster.\n2. After sucessfully installing slurm on cluster, ( /home , /apps ) directory of login instances will be shared with all other instances like controller and compute instances. So that all deep learning and python binaries must be installed inside `/home` or `/apps` directory.\n3. Installing python inside /home:\n\n```bash\ncd ~\n\nsudo yum -y update; sudo yum groupinstall \"Development Tools\"\n\nsudo yum -y install openssl-devel bzip2-devel libffi-devel\n\nmkdir tmp;cd tmp; wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz\n\ntar zxvf Python-3.8.3.tgz; cd Python-3.8.3\n\n./configure --enable-optimizations --prefix=$HOME/opt/python-3.8\n\nmake\n\nmake altinstall\n\nvi ~/.bashrc\n(add line: export PATH=$HOME/opt/python-3.8/bin:$PATH) at the end of .bashrc\n\nsource ~/.bashrc\n```\n\n```\n> which python3.8\n(output should be: /home/<username>/opt/python-3.8/bin/python3)\n\n# Now Lets install pytorch\n\n> curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n>  python3.8 get-pip.py\n\n> python3.8 -m pip install torch==1.5.0\n\n```\n\n4. Installing CUDA toolkits and Nvidia-drivers\n\n- Download the latest Nvidia CUDA [repository package](https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/) cuda-repo-rhel7-\\*.rpm.\n\n```bash\n> cd ~\n> wget https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-10.2.89-1.x86_64.rpm\n```\n\n- Install the CUDA repository package. This will enable CUDA repository on your CentOS 7 Linux system:\n\n```bash\n> sudo rpm -i cuda-repo-*.rpm\n```\n\n- Install cuda package from nvidia repository. Pytorch 1.5 works with cuda 10.2, so lets install cuda 10.2. Below command will install cuda and nvidia-drivers inside `/usr/local/cuda-10.2` directory.\n\n```bash\n> sudo yum install cuda-10-2\n```\n\n- By Now we know that only `/home` and `/apps` directory were shared across all other instances. So lets copy cuda directory from `/usr/local/` to `~/opt/cuda`\n\n```bash\n> mkdir ~/opt/cuda\n> cp -r /usr/local/cuda-10.2/* ~/opt/cuda/\n```\n\n- Export cuda path to Nvidia CUDA binary executables. Open the ~/.bashrc using your preferred text editor and add the following two lines\n\n```bash\nexport PATH=$HOME/opt/cuda/bin:$PATH\nexport LD_LIBRARY_PATH=$HOME/opt/cuda/lib64:$LD_LIBRARY_PATH\n```\n\n- Now Re-login or execute your updated ~/.bashrc file:\n\n```bash\n> source ~/.bashrc\n```\n\n- Now Confirm the CUDA installation:\n\n```bash\n> nvcc --version\n> nvidia-smi #( doesn't work as we dont have gpu in login instances )\n```\n\n> By Now your slurm system is ready and can run any training script using `sbatch` scheduling. Below I will discuss on how to compile nvidia-apex library. It takes me 2 days just to make nvidia-apex work on slurm cluster.If you follow my above steps exaclty, I am sure you will be able to install nvidia apex library inside your slurm cluster without any problems.\n\n# Installing Nvidia-Apex library inside your slurm cluster ( Optional )\n\nIf you don't know about Apex library, you can visit Nvidia-Apex [github page](https://github.com/NVIDIA/apex) to know more about Apex library. It is a set of utilites to help training model in mixed precision mode. It provide more numerical stable layer norm operations during training model in mixed precision mode, So that you can have both faster and stable training.\n\nFor `Nvidia-apex` to sucessfully compile, You need to upgrade your gcc --version to 7.3.0 .\n\n```bash\n> sudo yum install centos-release-scl\n> sudo yum install devtoolset-7-gcc*\n> scl enable devtoolset-7 bash\n> which gcc\n> gcc --version\n(above bash script must output gcc version as 7.3.0)\n```\n\nNow We will compile and install Apex-library inside python3.8\n\n```bash\n\n> git clone https://github.com/NVIDIA/apex\n> cd apex\n> export TORCH_CUDA_ARCH_LIST=\"6.0;7.0\"\n> CUDA_HOME=$HOME/opt/cuda python3.8 -m pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n```\n\nIts takes around 4 - 6 minutes to compile and install apex.\n\n# Distributed Training with pytorch and slurm\n\nI will not give details on how to setup codebase in pytorch for distributed training, Its a huge topic and might need another blog post for it only. Meanwhile, You can learn more about distributed training with pytorch [here](https://pytorch.org/tutorials/intermediate/dist_tuto.html). After you done setting up distributed training codebase in pytorch. You can start training using sbatch scripts.\n\n## Running the training Job Using SBATCH Script\n\nAfter we have done the cluster setup, preparing deep learning envs and building of the model, the last step is to finally run a job, i.e. to start the training. That is easily done by running sbatch script, which is basically a customized shell script. It effectively has two parts. The first part of the script is specific for the Slurm, it specifies the parameters for the Slurm job scheduler using the SBATCH command. The second part consists of bash (or some other shell) commands that you would normally run in terminal.\n\nBelow you will see demo SBATCH script (You need to modify sbatch script according to your needs).\n\n> inside run_training.sh file.\n\n```bash\n#!/bin/sh\n #SBATCH --job-name=distributed_training\n  #SBATCH --output=slurm_logs/slrm_stdout.%j\n  #SBATCH --error=slurm_logs/slrm_stderr.%j\n  #SBATCH --partition=gpu\n  ## make sure we don't clobber log files if jobs get restarted\n  #SBATCH --open-mode=append\n  #SBATCH --nodes=2\n  #SBATCH --time=24:00:00\n  ## make sure we are told about preempts, and jobs running out of time, 60s beforehand\n  #SBATCH --signal=USR1@60\n  #SBATCH --cpus-per-task=5\n  ## srun forks ntasks_per_node times on each node\n  #SBATCH --ntasks-per-node=1\n  #SBATCH --mem=200G\n  #SBATCH --gpus-per-node=1\n\npython3.8 train.py <Your training args here>\n```\n\nExecute the sbatch script using the sbatch command line:\n\n```bash\n> sbatch run_training.sh\n```\n\nRunning sbatch will return a Job ID for the scheduled job, for example:\n\n```\nSubmitted batch job 37\n```\n\nTo keep track of the job’s state, run squeue and to keep track of the cluster’s state, run sinfo:\n\n```bash\n> squeue\n\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n3 gpu hostname <username> CF 0:11 4 slurm-job-compute-0-[0-1]\n\n> sinfo\n\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST\ngpu* up infinite 4 mix# slurm-job-compute-0-[0-1]\ngpu* up infinite 6 idle~ slurm-job-compute-0-[2-7]\n```\n\n# Summary\n\nIn this blog post we learned how to setup slurm cluster in GCP ( which is super easy ), setup own deeplearning environment from installing python, pytorch to compiling apex from source and finally run training using sbatch script and keeping track of job state using `sinfo`, `squeue` and `scontrol`.\n\n# REFERENCES\n\n1. [https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine](https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine)\n2. [https://cloud.google.com/deployment-manager/docs/deployments](https://cloud.google.com/deployment-manager/docs/deployments)\n3. [https://pytorch.org/tutorials/intermediate/dist_tuto.html](https://pytorch.org/tutorials/intermediate/dist_tuto.html)\n4. [https://github.com/NVIDIA/apex](https://github.com/NVIDIA/apex)\n"},{"slug":"wandb-your-machine-learning-project","title":"Wandb Your machine learning project.","date":"2020-09-02 12:00","modified":"2020-09-02 12:00","category":"Blog","summary":"Wandb Tool for visualizing and tracking your machine learning experiments better than tensorboard.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python","authors":["Aaditya Chapagain"],"status":"published","content":"\nHave you ever worked on large Machine learning projects or research where you have to manage many experimentations ? often during large projects or experiments you have to log every nits and bits of your Machine Learning training. Some of us might already be there.\nRecently I was also working on large Machine learning Projects and it was very hard for me to track every experiments logs, visualization, experiments. There were lots of them and It would take lots of time to prepare presentation's of my experiments results to my peers and that's when I learned about wandb.\n\nWandb is API created by [Weights & Biases](https://www.wandb.com/) to collect, manage and visualize all your Machine learning experiments all at one place. Ohh, wait you must be wandering that **tensorboard** can also do these things right ?\n\nWell, there are lots of things which makes **wandb** a straight winner.\n\n1. Visually aesthetics UI.\n2. More manageable and customizable visualization and experiment tracking.\n3. Super easy to use API.\n4. Analyze system usage metrics alongside runs.\n5. Collaborate with team members\n6. Run parameter sweeps\n7. Keep records of experiments available forever\n\nI think the last point in above list is super cool, which is not feasible to do with tensorboard i.e You have to write huge amount of code by yourself to log these system wide metrics into tensorboard, but with wandb you can do this with just one line of code. and\n\nLets Dive into wandb API.\n\n# Installation\n\nInstalling `wandb` is very easy, just run below command in terminal and you are good to go.\n\n```bash\npip install wandb\n\npip install wandb --upgrade\n```\n\nOne thing I noticed during installing wandb is that, if we run only `pip install wandb` it will sometimes only install wrapper class of wandb. So, upgrading wandb library after installing it , worked for me.\n\n# Integration with your python code\n\nYou can use `wandb` with any deeplearning framework, either it is pytorch or Tensorflow.But, first you need to have account on [Weights & Biases](https://www.wandb.com/). After you create account on [Weights & Biases](https://www.wandb.com/) , you can get your wandb api keys from [setting](https://app.wandb.ai/settings). After you get your keys, you just need to write couple of code to integrate your deeplearning system with wandb logger.\n\n## Login\n\nFirst you need to login into your wandb using wandb API KEYS. you can login programetically or from terminal, but login using terminal is advised.\n\n```bash\n# This is secret and shouldn't be checked into version control\nexport WANDB_API_KEY=$YOUR_API_KEY\n\nwandb login\n```\n\nTo Programatically login you can use following code:\n\n```python\nWANDB_API_KEYS = \"<your api keys here>\"\n\nimport wandb\n\nwandb.login(WANDB_API_KEYS) # if you don't have $WANDB_API_KEYS in your env variable (Not Advised)\n\nwandb.login() # if $WANDB_API_KEYS is already been set.\n```\n\n## Initializing wandb ( wandb.init )\n\nBelow you will see simple wandb integration examples with keras.\n\n```python\n\n# initialize wandb with your project name and optionally with configutations.\nwandb.init(project='demo-keras-integration', name = 'first_run'\n           config={\n              \"learning_rate\": 0.005,\n              \"epochs\": 25,\n              \"batch_size\": 64,\n              \"loss_function\": \"sparse_categorical_crossentropy\",\n              \"architecture\": \"CNN\",\n              \"dataset\": \"CIFAR-10\",\n           }, anonymous='never')\nconfig = wandb.config\n\n# Initialize model like you usually do.\ntf.keras.backend.clear_session()\nmodel = Model()\nmodel.summary()\n\n# compile model like you usually do.\n# notice use of config.\noptimizer = tf.keras.optimizers.Adam(config.learning_rate)\nmodel.compile(optimizer, config.loss_function, metrics=['acc'])\n\n```\n\n`project` is name of your project and your project might have different run, so `name` parameter will distinguish one run from other.\nIf you run above code and get back to your wandb dashboard it will new project called `demo-keras-integration` and with `first_run` in it.\n\n[![wandb Init](/images/wandb-init.png)](https://ibb.co/hDjwC6s)\n\nAbove init configuration will create new run everytime you call `wandb.init` But sometimes that not what you want, If your training is preemptible and might takes days or months to complete then you can resume from previous logged metrics by providing `resume = True` parameter to `wandb.init` and you will also need to set unique id, to distinguish one run from another. Below code will resume your logs with current run in your wandb dashboard even after you run `init` multiple times.\n\n```python\n\nwandb.init(project='demo-keras-integration', name = 'first_run', resume= True,\n            id = 'my_first_run'\n           config={\n              \"learning_rate\": 0.005,\n              \"epochs\": 25,\n              \"batch_size\": 64,\n              \"loss_function\": \"sparse_categorical_crossentropy\",\n              \"architecture\": \"CNN\",\n              \"dataset\": \"CIFAR-10\",\n           }, anonymous='never')\n```\n\n## Train with Wandb callback\n\nwandb made easy to log your model metrics into your project space by providing various callback function to log your metrics directly into wandb dashboard, without writing extra code.\n\n```python\n\nfrom wandb.keras import WandbCallback\n\n# train with our favorite model.fit\n# notice WandbCallback used as a regular callback\n# notice the use of config\n_ = model.fit(x_train, y_train,\n          epochs=config.epochs,\n          batch_size=config.batch_size,\n          validation_data=(x_test, y_test),\n          callbacks=[WandbCallback()])\n\n```\n\nIf you are working on images and need a way to log your correctly classified and misclassified sample images, you can also do so using wandb. Below you will see the examples of it.\n\n```python\n\n# in order to get prediction on small subset of images.\nval_images, val_labels = x_test[:32], y_test[:32]\n\n# train with our favorite model.fit\n# notice WandbCallback used as a regular callback\n# notice that we are passing in some arguments as well\n# notice the use of config\n_ = model.fit(x_train, y_train,\n          epochs=config.epochs,\n          batch_size=config.batch_size,\n          validation_data=(x_test, y_test),\n          callbacks=[WandbCallback(data_type='image',\n                                   training_data=(val_images, val_labels),\n                                   labels=CLASS_NAMES)])\n```\n\n[![logging images with wandb](/images/wand-log-images.png)](https://ibb.co/ZcbvNP9)\n\n## Log Custom metrics with wandb.log\n\nYou can always log custom metrics and extra information using wandb.log .\n\n```python\nloss, accuracy = model.evaluate(x_test, y_test)\nprint('Test Error Rate: ', round((1-accuracy)*100, 2))\n\n# notice the use of wandb.log.\n# We can easiy pass in values as key-value pairs.\nwandb.log({'Test Error Rate': round((1-accuracy)*100, 2)})\n\n```\n\n# wandb Dashboard\n\n[![Wandb dashboard](/images/wandb-dashboard.png)](https://ibb.co/tBZDJzW)\n\nYou can see all your logged metrics in your wandb project dashboard. Wandb Groups information into different sections like Overview, Charts ,logs, system, model and files.\n\n## Overview\n\n[![Wandb dashboard](/images/wandb-overview.png)](https://ibb.co/RvNFHTW)\n\nThe information in the `overview` section is pretty intuitive and self-explanatory. However, the Git Repository field and the Git State field are worthy of special mention. You can run the checkout command in the Git State field to pin down the exact code for reproducing the experiment. Under the hood, wandb tracks all the changes you made to the original repo, and save the \"diff\" files in a local directory. In this way, you can easily switch between different versions of the code without manually pushing the change to the remote repo.\n\n## Logs\n\nThe `logs` section shows the console output during the experiment. This is useful for debugging the performance of the model.\n\n[![Wandb dashboard](/images/wandb-logs.png)](https://ibb.co/hmKFC7H)\n\n## Systems\n\nTo me, the system section is where wandb really shines and separates itself from other options such as TensorBoard. It is a centralized place to track system utilization during the experiment. There are in total of 8 graphs displayed in this section. These graphs give you insight into the training bottleneck and possible ways to uplift it. For example, below are the diagrams of the experiment:\n\n[![Wandb dashboard](/images/wandb-system.png)](https://ibb.co/VNGQbyQ)\n\n## Tensorboard\n\nwandb also provide options for people who love tensorboard. You can directly sync wandb with tensorboard by just setting `sync_tensorboard = True` in your `wandb.init`. So, that every information that is logged into tensorboard will also be logged into wandb.\n\n```python\n\nwandb.init(\n    name = '<your run name>',\n    project = 'your project name',\n    config = '<your config>',\n    sync_tensorboard = True)\n```\n\n# Summary\n\nWe discuss how to integrate wandb in any deeplearning framework using python for inspecting the efficiency of training jobs. We also looked into other aspects of wandb that makes it so much unique than other training logging software like tensorboard.\n\nTo learn more about wandb, check out their website: [https://www.wandb.com/](https://www.wandb.com/).\n"},{"slug":"speech-signal-processing-using-python","title":"Speech Signal Processing using python","date":"2020-08-11 08:00","modified":"2020-08-11 08:00","category":"Blog","summary":"Signal Processing and Speech Recognition using python","tags":"signal processing, speech recognition, ASR, machine learning, Deep Learning.","authors":["Aaditya Chapagain"],"status":"published","content":"\n## Table of contents\n\nSpeech processing is very first phase in any speech system either it is speech recognition system or speaker Diarization or something else. Speech processing plays an important role in speech system to extract vocal features i.e identify the components of the audio signal that are good for identifying the linguistic content and discarding all other stuff which carries information like background noise, emotion etc.\n\nIn this post we will learn very important mathematical concept about speech processing in any speech system and implement the mathematics in python.\n\nMel Frequency Cepstral Coefficents (MFCCs) and Filter Banks are a feature widely used in automatic speech and speaker recognition. But Filter Banks is more popluar Nowadays due to its robustness on mapping vocal features.Computing filter banks and MFCCs involve somewhat same procedure, where in both cases filter banks are computed and with a few more extra steps MFCCs can be obtained.\n\nLet's get started with loading speech signal with python. I will be using python 3.6 for this post.\n\n```python\n\nimport numpy as np\nfrom scipy.io import wavfile\nfrom scipy.fftpack import dct\nfrom matplotlib import pyplot as plt\n\nsample_rate, signal = wavfile.read('../audio/mfcc.wav')\n\nsignal = signal[0:int(10* sample_rate)]\nTime = np.linspace(0, len(signal) / sample_rate, num=len(signal))\n\nplt.plot(Time, signal)\n```\n\nThe raw signal has the following from in the time domain:\n[![raw signal image of speech](/images/spectral-image-speech.png)](https://ibb.co/nPvnTGc)\n\n## Pre-Emphasis\n\nThe first step is to apply a pre-emphesis filter on signal to amplify the high frequencies. A pre-emphesis filter is useful in several ways:\n\n- balance frequency spectrum since high frequencies usually have smaller magnitudes compared to lower frequencies\n- avoid numerical problems during fourier operation\n- Might also improve the signal to Noise Ratio (SNR)\n\nThe pre-emphesis filter can be applied to a signal x using the first order filter in the following equation:\n\n$$ y(t) = x(t) - \\alpha x(t-1) $$\n\nwhich can be easily implemented using the following line, where typical values for the filter coefficeint ( $\\alpha$ ) are 0.95 to 0.97,\n`pre_empasis = 0.97`\n\n```python\n\npre_emphasis = 0.97\nemphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n\n```\n\nPre-emphasis has a modest effect in modern systems, mainly because most of the motivations for the pre-emphasis filter can be achieved using mean normalization except for avoiding the Fourier transfrom numerical issues which should not be a problem in modern FFT implementations.\n\nThe signal after pre-emphasis has the following effect in orignal signal.\n\n[![before and after pre-emphesis](/images/before-after-preemphesis.png)](https://ibb.co/Jqn0x33)\n\n## Framing\n\nAfter pre-emphasis, we need to split the signal into short-time frames. The rationale behind this step is that frequencies in a signal change over time, so in most cases it dosen't make sense to do the Fourier Transform across the entier signal in that we would lose the frequency contours of the signal over time. To avoid that, we can safely assume that frequencies in a signal are stationary over a very short period of time. Therefore, by doing a Fourier transform over this short-time frame, we can obtain a good approximation of the frequency contours of the signal by concatenating adjacent frames.\n\nTypical frame sizes in speech processing rnage from 20 ms to 40 ms with 50% (+/- 10%) overlap between conseutive frames.Popular settings are 25 ms for the frame size, frame_size = 0.025 and a 10 ms stride (15 ms overlap), frame_stride = 0.01\n\n```python\n\nframe_size = 0.025\nframe_stride = 0.01\n\nframe_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  # Convert from seconds to samples\nsignal_length = len(emphasized_signal)\nframe_length = int(round(frame_length))\nframe_step = int(round(frame_step))\nnum_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n\npad_signal_length = num_frames * frame_step + frame_length\nz = np.zeros((pad_signal_length - signal_length))\npad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n\nindices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\nframes = pad_signal[indices.astype(np.int32, copy=False)]\n\n```\n\n## Window\n\nAfter slicing the signal into frames, we apply a window function such as the Hamming window to each frame. A Hamming window has the following form:\n$$ w[n] = 0.54 − 0.46 cos ( \\frac{2πn}{N − 1} ) $$\nwhere,  0 $\\leq$ n $\\leq$ N-1,N is the window length.\n\nThere are several reasons why we need to apply a window function to the frames, notably to counteract the assumption made by the FFT that the data is infinite and to reduce spectral leakage.\n\n````python\n\nframes *= np.hamming(frame_length)```\n# frames *= 0.54 - 0.46 * np.cos((2 * np.pi * n) / (frame_length - 1))  # Explicit Implementation **\n````\n\n## Fourier-Transfrom and Power Spectrum\n\nWe can now do an N-point FFT on each frame to calculate the frequency spectrum, which is also called Short-Time-Fourier-Transfrom (STFT), where N is typically 256 or 512, `NFFT = 512`; and then compute the power spectrum (Periodogram) using the following equation:\n$$P = \\frac{|FFT(x_i)|^2}{N}$$\nwhere , $x_i$ is the $i^{th}$ frame of signal $x$. This can be easily imlemented with the following lines:\n\n```python\nNFFT = 512\n\nmag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\npow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n```\n\n## Filter Banks\n\nThe final step to computing filter banks is applying triangular filters, typically 40 filters, nfilt = 40 on a Mel-scale to the power spectrum to extract frequency bands. The Mel-Scale aims to mimic the non-linear human ear perception of sound, by being more discriminative at lower frequenceis and less discriminative at higher frequencies. We can convert between Hertz ($f$) and Mel ( $m$) using the following equations:\n$$ \\Large m = 2595 \\log\\_{10} (1 + \\frac{f}{700}) \\approx 1125 \\ln (1 + \\frac{f}{700} ) $$\n\n$$ \\Large f = 700 ( 10^{ \\frac{m}{2595} } -1)$$\n\nEach filter in the filter bank is triangular having a response of 1 at the center frequency and decrease linearly towards 0 till it reaches the center frequencies of the two adjacent filters where the response is 0. which can be modeled by the following equation:\n\n$$\nH_m(k) =\n  \\begin{cases}\n      \\ 0                                      & k < f(m - 1) \\\\\n      \\\\\n      \\dfrac{k - f(m - 1)}{f(m) - f(m - 1)}  & f(m - 1) \\leq k < f(m) \\\\\n      \\\\\n      1                                      & k = f(m) \\\\\n      \\\\\n      \\dfrac{f(m + 1) - k}{f(m + 1) - f(m)}  & f(m) < k \\leq f(m + 1) \\\\\n      \\\\\n      0                                      & k > f(m + 1) \\\\\n  \\end{cases}\n$$\n\n```python\nnfilt = 40\n\nlow_freq_mel = 0\nhigh_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))  # Convert Hz to Mel\nmel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\nhz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\nbin = np.floor((NFFT + 1) * hz_points / sample_rate)\n\nfbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\nfor m in range(1, nfilt + 1):\n    f_m_minus = int(bin[m - 1])   # left\n    f_m = int(bin[m])             # center\n    f_m_plus = int(bin[m + 1])    # right\n\n    for k in range(f_m_minus, f_m):\n        fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n    for k in range(f_m, f_m_plus):\n        fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\nfilter_banks = np.dot(pow_frames, fbank.T)\nfilter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\nfilter_banks = 20 * np.log10(filter_banks)  # dB\n```\n\nAfter applying the filter bank to the power spectrum (peridogram) of the signal, we obtain the following spectrogram:\n\n```python\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\ncax = ax.matshow(\n    np.transpose(filter_banks),\n    interpolation=\"nearest\",\n    aspect=\"auto\",\n    cmap=plt.cm.afmhot_r,\n    origin=\"lower\",\n)\nfig.colorbar(cax)\nplt.title(\"Mel compression Spectrogram\")\nplt.show()\n```\n\n[![melspectrogram of speech signal](/images/mfcc-spectorgram.png)](https://ibb.co/xSyfhsV)\n\n## Mel-frequency cepstral Coecfficents (MFCCs)\n\nIt turns out that filter bank coefficients computed in the previous step are higly correlated, which could be problematic in some machine learning algorithms. Therefore, we can apply Discrete Cosine Transform(DCT) to decorrelate the filter bank coefficients and yield compressed representation of filter banks. Typically, for Automatic SPeech Recognition (ASR), the resulting cepstral coefficeints 2- 13 are retained and the rest are discareded; `num_ceps = 12`. The reasons for discarding the other coefficeints is that they represent fast changes in the filter bank coefficients and these fine details don't contribute to ASR.\n\n```python\nnum_ceps = 12\nmfcc = dct(filter_banks, type = 2, axis=1, norm=\"ortho\")[:,1: (num_ceps + 1)] # keep 2-13\n\n```\n\nOne may apply sinusoidal liftering to the MFCCs to de-emphasize higher MFCCs which has been claimed to improve speech recognition in noisy signals.\n\n```python\n\ncep_lifter = 22\n(nframes, ncoeff) = mfcc.shape\nn = np.arange(ncoeff)\nlift = 1 + (cep_lifter / 2) * np.sin(np.pi * n/ cep_lifter)\nmfcc *= lift\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\ncax = ax.matshow(\n    np.transpose(mfcc),\n    interpolation=\"nearest\",\n    aspect=\"auto\",\n    cmap=plt.cm.afmhot_r,\n    origin=\"lower\",\n)\nfig.colorbar(cax)\nplt.title(\"MFCC Spectrogram\")\nplt.show()\n```\n\n[![MFCC spectrogram of speech signal](/images/mel-spectrogram.png)](https://ibb.co/S7c5pnL)\n\n## Mean Normalization\n\nAs previously mentioned, to balance the spectrum and improve the Signal-to-Noise (SNR), we can simply substract the mean of each coefficeint from all frames,\n\n```python\nfilter_banks -= (np.mean(filter_banks, axis = 0) + 1e-8)\n```\n\nand similarly for MFCCs:\n\n```python\n\nmfcc -= (np.mean(mfcc, axis = 0) + 1e-8)\n\n```\n\nTo this point, the steps to compute filter banks and MFCCs were discussed in terms of their motivation and implementations.It is interesting to note that all steps needed to compute filter banks were motivated by the nature of the speech signal and the human perception of such signals.\n\nOn the Contrary, the extra steps needed to compute MFCCs were motivated by the limitation of some classifical machine learning algorithms.The Discrete cosine Transfrom (DCT) was needed to decorrelate filter banks coeffiicients, a process refer as whitening. In particular, MFCCs were very popular with Gaussian Mixture Models - Hidden Markov Models(GMM - HMM).\nBut with the advent of Deep learning in speech system, one might not need DCT.\n"},{"slug":"testing-in-go-nutshell","title":"Testing Go Program in Nutshell.","date":"2020-08-11 08:00","modified":"2020-08-11 08:00","category":"Blog","summary":"Testing Framework in Go is extremely simple and minimal","tags":"Go, Golang, Test, Testing, software development, Programming language, TDD","authors":["Aaditya Chapagain"],"status":"published","content":"\n## Table of contents\n\nTesting, by which we implicitly mean _automated testing_, is the practice of writing small programs that check that the code under test ( the production code ) behaved as expected for certain input pools, which is usually either carefully chosen to exercise certain features or randomized to ensure broad coverage.\n\nGo's approach to testing seems rather low-tech in comparison to some big language like java, c++. It relies on one command, `go test`, and a set of conventions for writing test functions that go test can run. The comparatively lightweight mechanism is effective for pure testing, and it extends naturally to benchmarks and systematic example for documentation. The best thing about writing test code in Go is , that test code is no different from code if we intend to implement API we are testing.We focus on short functions that focus on one part of the task.We have to be careful about boundary conditions. think about data structures, and reason about what results a computation should produce from suitable inputs. But is the same as writing ordinary Go code.\n\n# The GO test Tool\n\nTesting in Go starts with `go test` subcommand i.e. test driver for Go packages that are organized according to certain conventions. `go test` command specifically looks and execute files that ends with **\\_test.go** which are not part of the package when built by `go build `.\n\nThe Go test files contains three kinds of special functions :\n\n- A _test function_: Function which name starts with `Test`. This function exercises some program logic for correct behaviour; subcommand `go test` calls the test function and reports the result which is either `PASS` or `FAIL`.\n\n- A _benchmark function_: It has the name beginning with `Benchmark` and measures the performance of some operations; subcommand `go test` reports the mean execution time of the operation.\n\n- An _example function_: Its name starts with `Example` provides machine-checked documentation.\n\nThe `go test` tool scans the \\*\\_test.go files for these special functions, generates a temporary main package that calls them all in the proper way, builds and runs it, reports the result, and then cleans up.\n\n# Test Functions\n\nEach test file must import the testing package. Test functions has following signature :\n\n```go\nfunc TestName(t *testing.T) {\n  // ..\n}\n\n```\n\nTest function names must begin with Test; The optional suffix Name must begin with a capital letter:\n\n```go\nfunc TestSin(t *testing.T) { // ... }\nfunc TestCos(t *testing.T) { // ... }\n```\n\nThe t parameter provides methods for reporting test failures and logging additional information. Lets create a new package palindrome containing a single function IsPalindrome that reports whether a string a string reads same forward and backward.\n\n```go\npackage palindrome\n\nfunc IsPalidrome(s string) bool {\n  for i := range s {\n    if s[i] != s[len(s) - 1 - i] {\n      return false\n    }\n  }\n  return true\n}\n\n```\n\nIn the same directory, the palindrome_test.go contains two test functions named TestPalindrome and TestNonPalindrome. Each check that IsPalindrome gives the right answer for a single input and reports failures using t.Errorf:\n\n```go\npackage palindrome\n\nfunc TestPalindrome(t *testing.T) {\n  if !IsPalindrome(\"detartrated\") {\n    t.Errorf(`IsPalindrome(\"detartrated\") = false`)\n  }\n  if !IsPalindrome(\"kayak\") {\n    t.Errorf(`IsPalindrome(\"kayak\") = false`)\n  }\n}\n\nfunc TestNonPalindrome(t *testing.T) {\n  if IsPalindrome(\"palindrome\") {\n    t.Error(`IsPalindrome(\"palindrome\") = true`)\n  }\n}\n\n```\n\nNow,we already have test code to test our functionality, we can use go test command to run tests in go.\nRemember `go test palindrome` will run the package level test.For this to work you have to place your package in either `$GOROOT` or `$GOPATH` directory.\nIn our case we can test our functionlity by going into pacakge directory.\n\n```bash\n> cd <path_to_project_dir>/palindrome\n> go test\n\nPASS\nok  \t<path_to_project_dir>/palindrome\t0.001s\n```\n\nTo look into more details on test function, To see which one failed and which one succeed, we can use `-v` flag , which will prints the name and execution time of each test in the package.\n\n```bash\n> go test -v\n\n=== RUN TestPalindrome\n--- PASS: TestPalindrome (0.00s)\n=== RUN TestNonPalindrome\n--- PASS: TestNonPalindrome (0.00s)\n=== RUN TestFrenchPalindrome\n--- FAIL: TestFrenchPalindrome (0.00s)\nword_test.go:28: IsPalindrome(\"été\") = false\n=== RUN TestCanalPalindrome\n--- FAIL: TestCanalPalindrome (0.00s)\nword_test.go:35: IsPalindrome(\"A man, a plan, a canal: Panama\") = false\nFAIL\nexit status 1\nFAIL  <path_to_project_dir>/palindrome 0.001s\n\n```\n\nand the -run flag, whose argument is a regular expression, causes go test to run only those tests whose function name matches the patter:\n\n```bash\n\n> go test -v -run=\"French|Canal\"\n=== RUN TestFrenchPalindrome\n--- FAIL: TestFrenchPalindrome (0.00s)\nword_test.go:28: IsPalindrome(\"été\") = false\n=== RUN TestCanalPalindrome\n--- FAIL: TestCanalPalindrome (0.00s)\nword_test.go:35: IsPalindrome(\"A man, a plan, a canal: Panama\") = false\nFAIL\nexit status 1\nFAIL  <path_to_project_dir>/palindrome 0.001s\n\n```\n\n# Benchmark Function\n\nBenchmarking is the practice of measuring the performance of a program on a fixed workload. In Go, a benchmark function look like a test function, but with the\nBenchmark prefix and a *testing.B parameter that provides most of the same methods as a *testing.T, plus few extra related performance measurement.It also exposes\nan Integer field N, which specifies the number of times to perform the operation being measured.\n\nHere's a benchmark for IsPalindrome that calls it N times in a loop.\n\n```go\nfunc BenchmarkIsPalindrome(b *testing.B) {\n  for i := 0; i < b.N; i++ {\n    IsPalindrome(\"A man, a plan, a canal: Panama\")\n  }\n}\n```\n\nUnlike tests, by default no benchmarks are run. The argument to the -bench flag selects which benchmark to run. It is regular expression matching the names of Benchmark functions, with a default value that matches none of the functions. The \".\" pattern causes it to match all benchmark functions in package.\n\n```bash\n> go test -bench=.\ngoos: linux\ngoarch: amd64\nBenchmarkIsPalindrome-12    \t 4377900\t       301 ns/op\nPASS\nok  \t<path_to_project_dir>/palindrome\t1.598s\n```\n\nThe benchmark name’s numeric suffix, 12 here , indicates the value of GOMAXPROCS, which is important for concurrent benchmarks.\n\nThe report tells us that each call to _IsPalindrome_ took about 0.301 microseconds, averaged over 4377900 runs. Since the benchmark runner initially has no idea how long the operation takes, it make some initial measuremetns using small values of N and then extrapolates to a value large enough for a stable timing measurement to be made.\n"}]},"__N_SSG":true}