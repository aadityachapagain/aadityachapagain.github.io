{"pageProps":{"post":{"slug":"distributed-training-with-slurm-on-gcp","title":"Distributed Training of Deep Learning model with Slurm on GCP","date":"2020-09-15 12:00","modified":"2020-09-15 12:00","category":"Blog","summary":"Faster Training of Large Deep Learning models is much easier than you think with the help of Slurm.","tags":"Datasets, Machine Learning, Visualization, training, Deep Learning, ML tools, tensorboard, python, slurm, distributed training, High performance computing, HPC, parallel processing, tensorflow, Pytorch, DL, Language model,","authors":["Aaditya Chapagain"],"status":"published","content":"<p>Recently, I was working on Big Machine Learning project. The task was to pretraining Large Machine learning models (with parameter in the range of several Billion ). And Normal training approch didn't work ( obviously ).With 8 GPU Volta core machines, it would take several months to complete just 1 epcoh of training, that's the point when i think of distributed training. I was using gcp ( google cloud ) for training models and found out that google already have support for High Performance Computing with Slurm . You can find Minimal working example on slurm from google codelabs here <a href=\"https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\">https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine</a>.</p>\n<p>Through this blog, I will try to explain what is HPC? , Why HPC ?, how can we train large Deep Learning models with slurm.</p>\n<h1 id=\"what-is-hpc-\">What is HPC ?</h1>\n<p>High Performance Computing (HPC) is the use of supercomputers and parallel processing techniques for solving complex mathematical and computational problems. HPC technology primarily focuses on developing parallel processing algorithms and systems by incorporating both administration and parallel computational techniques. HPC is typically used for solving advanced problems that require a lot time .</p>\n<h1 id=\"why-hpc-\">Why HPC ?</h1>\n<p>When you have loads of data and its processing takes really long time, then the approch <strong>divide et impera</strong> comes at hand.</p>\n<p>With HPC, we can divide any job so that every node processes different partitions of the data in parallel, speeding up the execution time.</p>\n<h1 id=\"slurm-workload-manager-on-gcp\">Slurm Workload Manager on GCP</h1>\n<p>To make computing with slurm easier in GCP, Google and SchedMD ( Slurm's Creators ) joined forces and as a result, we can run a Slurm cluster on Google Cloud Platform. We don't have to worry about the parallel computing techniques since slurm takes care of that and GCP takes care of setting up a cluster and providing resources.\nBasic architectural diagram of a stand-alone Slurm Cluster in Google Cloud Platform.</p>\n<p><a href=\"https://ibb.co/SPhjRyM\"><img src=\"/images/slurm-archi.png\" alt=\"Slum Architecture in GCP\"></a></p>\n<p>As we can see in above pictures, Slurm cluster contains three types of nodes: <strong>login</strong>, <strong>controller</strong> and <strong>Compute</strong> node.</p>\n<ul>\n<li><strong>Login Node</strong> serves as an interface for the user: user should communicate with the cluster exclusively through the login node (starting the job, requiring resources, ...)</li>\n<li><strong>Controller Node</strong> manages resources and job scheduling for the user.</li>\n<li><strong>Compute Node</strong> executes the job.</li>\n</ul>\n<h1 id=\"setting-up-a-slurm-cluster-on-gcp\">Setting Up a Slurm Cluster on GCP</h1>\n<p>Before describing the setup, let us explain in short how does GCP implement a slurm cluster.</p>\n<p>In GCP, a cluster is realized as a <strong>deployment</strong>. A deployment is an instantiation of a set of resources that are defined in a configuration. A deployment can contain a number of resources, across a variety of Google Cloud Platform services. When you create a deployment, Deployment Manager creates all of the described resources in the respective Google Cloud Platform APIs.</p>\n<p>This brings us to the cluster's nodes. Each node in a cluster is actually a <strong>Virtual Machine</strong>.When a deployment is created, three new virtual machines appear in \"VM instances\" page, under \"Compute Engine\". Those VMs are login instances, Controller instances, and compute image instance.</p>\n<p>Compute Instance is a bit trickey part. One thing to notice is that deployment does not create compute instance,but exactly one compute image instance even if you request more compute nodes in your cluster. So, if a user requests 10 compute nodes for the cluster, those 10 virtual machines will not be immediately instantiated with the cluster deployment. Here's what is happening. These compute instances are created in the later step when you run a job and request the number of nodes for the job. Then the compute nodes will be allocated, and they will appear in the \"VM Instances\" page. Shortly after the job is completed, these virtual machines will be deallocated and will disappear from the list. This way a user gets new compute VMs everytime. The fact that deployment create compute image instance rather than compute nodes directly is that, you might not be using compute node all the time and creating compute nodes unnecessarily might affect your billing, so , slurm will create new compute nodes and use compute image instance as a templete to dynamically create new instance during running jobs, so that you will be billed for exact time period your compute node will run.</p>\n<p>Below you can see visual representation of the described process:</p>\n<p><a href=\"https://ibb.co/mFrGDy3\"><img src=\"/images/slurm-hpc.png\" alt=\"Slum Architecture in GCP\"></a></p>\n<p>Finally, let's head to the cluster setup. In this blog post, we will setup Slurm cluster for training Deep Learning Model with several nodes.Customize the information so that they will suit your needs:</p>\n<ol>\n<li>Launch Google Cloud Shell</li>\n<li>Check that you already authenticated and that the project is already set to your <strong>PROJECT_ID</strong>:</li>\n</ol>\n<pre><code class=\"hljs language-bash\">$ gcloud auth list\n\nCredentialed accounts:\n&#x3C;your email>\n\n$ gcloud config list project\n[core]\nproject = &#x3C;PROJECT_ID>\n\n</code></pre>\n<ol>\n<li>Clone git repository that contains the Slurm Google Cloud Platform deployment-manager files:</li>\n</ol>\n<pre><code class=\"hljs language-bash\">$ git <span class=\"hljs-built_in\">clone</span> https://github.com/SchedMD/slurm-gcp.git\n\n</code></pre>\n<ol start=\"2\">\n<li>Switch to the Slurm deployment configuration directory:</li>\n</ol>\n<pre><code class=\"hljs language-bash\">$ <span class=\"hljs-built_in\">cd</span> slurm-gcp\n</code></pre>\n<ol start=\"3\">\n<li>Configure the Slurm Deployment YAML file. Provide information that suits your needs. There are plenty more parameters available, they can be found in SchedMD's GitHub repository. Below is the script that was sufficient for my needs.</li>\n</ol>\n<pre><code class=\"hljs language-yaml\">\n<span class=\"hljs-comment\"># [START cluster_yaml]</span>\n<span class=\"hljs-attr\">imports:</span>\n<span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\">slurm.jinja</span>\n\n<span class=\"hljs-attr\">resources:</span>\n<span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name:</span> <span class=\"hljs-string\">slurm-cluster</span>\n<span class=\"hljs-attr\">type:</span> <span class=\"hljs-string\">slurm.jinja</span>\n<span class=\"hljs-attr\">properties:</span>\n    <span class=\"hljs-attr\">cluster_name            :</span> <span class=\"hljs-string\">slurm-job</span>\n\n    <span class=\"hljs-attr\">zone                    :</span> <span class=\"hljs-string\">us-central1-b</span>\n    <span class=\"hljs-attr\">controller_machine_type :</span> <span class=\"hljs-string\">n1-standard-2</span>\n    <span class=\"hljs-attr\">controller_disk_type      :</span> <span class=\"hljs-string\">pd-standard</span>\n    <span class=\"hljs-attr\">controller_disk_size_gb   :</span> <span class=\"hljs-number\">50</span>\n    <span class=\"hljs-attr\">external_controller_ip    :</span> <span class=\"hljs-literal\">True</span>\n\n    <span class=\"hljs-attr\">login_machine_type        :</span> <span class=\"hljs-string\">n1-standard-2</span>\n    <span class=\"hljs-attr\">login_disk_type           :</span> <span class=\"hljs-string\">pd-standard</span>\n    <span class=\"hljs-attr\">login_disk_size_gb        :</span> <span class=\"hljs-number\">50</span>\n    <span class=\"hljs-attr\">external_login_ips        :</span> <span class=\"hljs-literal\">True</span>\n\n    <span class=\"hljs-attr\">compute_image_machine_type  :</span> <span class=\"hljs-string\">n1-standard-2</span>\n    <span class=\"hljs-attr\">compute_image_disk_type   :</span> <span class=\"hljs-string\">pd-standard</span>\n    <span class=\"hljs-attr\">compute_image_disk_size_gb:</span> <span class=\"hljs-number\">200</span>\n    <span class=\"hljs-attr\">external_compute_ips      :</span> <span class=\"hljs-literal\">False</span>\n\n    <span class=\"hljs-attr\">partitions :</span>\n    <span class=\"hljs-bullet\">-</span> <span class=\"hljs-attr\">name           :</span> <span class=\"hljs-string\">gpu</span>\n        <span class=\"hljs-attr\">machine_type   :</span> <span class=\"hljs-string\">n1-standard-16</span>\n        <span class=\"hljs-attr\">max_node_count :</span> <span class=\"hljs-number\">10</span>\n        <span class=\"hljs-attr\">zone           :</span> <span class=\"hljs-string\">us-central1-b</span>\n        <span class=\"hljs-comment\"># cpu_platform           : Intel Skylake</span>\n        <span class=\"hljs-attr\">preemptible_bursting   :</span> <span class=\"hljs-literal\">True</span>\n        <span class=\"hljs-attr\">compute_disk_type      :</span> <span class=\"hljs-string\">pd-standard</span>\n        <span class=\"hljs-attr\">compute_disk_size_gb   :</span> <span class=\"hljs-number\">200</span>\n        <span class=\"hljs-attr\">gpu_type               :</span> <span class=\"hljs-string\">nvidia-tesla-v100</span>\n        <span class=\"hljs-attr\">gpu_count              :</span> <span class=\"hljs-number\">1</span>\n\n<span class=\"hljs-comment\">#  [END cluster_yaml]</span>\n\n</code></pre>\n<ol start=\"4\">\n<li>In the Cloud shell Session, execute the following command from the slurm-gcp folder:</li>\n</ol>\n<pre><code class=\"hljs language-bash\">> gcloud deployment-manager deployments create slurm-deployment  --config  slurm-cluster.yaml\n</code></pre>\n<pre><code>This command creates a deployment  named slurm-deployment. The operation can take few minutes to complete.\n</code></pre>\n<p>5. Verify the deployment ( Navigation menu --> Deployment Manager)\n6. Verify the cluster's instances ( Navigation menu --> Compute Engine --> VM Instances) There should be login , controller, and compute image instances. Compute image instance will live for small amount of time for setting up compute images, after that it will be off. and Compute Instances show up only when you allocate them for the sbatch job. they disappear shortly the job is completed.</p>\n<ol start=\"7\">\n<li>\n<p>Log in to login instance .</p>\n<p>While logging in to the login instances if <em>Slurm is currently being installed/configured in the background.</em> Don't install any packages during that time, as that might disrupt the installation process of slurm. Wait for around 10 for slurm to be installed fully and you can log in to login instances and do whatever you want.</p>\n</li>\n</ol>\n<h1 id=\"setting-up-deep-learning-environment-inside-slurm-cluster\">Setting up Deep Learning Environment inside slurm cluster</h1>\n<ol>\n<li>SSH to login instance in your slurm cluster.</li>\n<li>After sucessfully installing slurm on cluster, ( /home , /apps ) directory of login instances will be shared with all other instances like controller and compute instances. So that all deep learning and python binaries must be installed inside <code>/home</code> or <code>/apps</code> directory.</li>\n<li>Installing python inside /home:</li>\n</ol>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">cd</span> ~\n\nsudo yum -y update; sudo yum groupinstall <span class=\"hljs-string\">\"Development Tools\"</span>\n\nsudo yum -y install openssl-devel bzip2-devel libffi-devel\n\n<span class=\"hljs-built_in\">mkdir</span> tmp;<span class=\"hljs-built_in\">cd</span> tmp; wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz\n\ntar zxvf Python-3.8.3.tgz; <span class=\"hljs-built_in\">cd</span> Python-3.8.3\n\n./configure --enable-optimizations --prefix=<span class=\"hljs-variable\">$HOME</span>/opt/python-3.8\n\nmake\n\nmake altinstall\n\nvi ~/.bashrc\n(add line: <span class=\"hljs-built_in\">export</span> PATH=<span class=\"hljs-variable\">$HOME</span>/opt/python-3.8/bin:<span class=\"hljs-variable\">$PATH</span>) at the end of .bashrc\n\n<span class=\"hljs-built_in\">source</span> ~/.bashrc\n</code></pre>\n<pre><code>> which python3.8\n(output should be: /home/&#x3C;username>/opt/python-3.8/bin/python3)\n\n# Now Lets install pytorch\n\n> curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n\n>  python3.8 get-pip.py\n\n> python3.8 -m pip install torch==1.5.0\n\n</code></pre>\n<ol start=\"4\">\n<li>Installing CUDA toolkits and Nvidia-drivers</li>\n</ol>\n<ul>\n<li>Download the latest Nvidia CUDA <a href=\"https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/\">repository package</a> cuda-repo-rhel7-*.rpm.</li>\n</ul>\n<pre><code class=\"hljs language-bash\">> <span class=\"hljs-built_in\">cd</span> ~\n> wget https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-repo-rhel7-10.2.89-1.x86_64.rpm\n</code></pre>\n<ul>\n<li>Install the CUDA repository package. This will enable CUDA repository on your CentOS 7 Linux system:</li>\n</ul>\n<pre><code class=\"hljs language-bash\">> sudo rpm -i cuda-repo-*.rpm\n</code></pre>\n<ul>\n<li>Install cuda package from nvidia repository. Pytorch 1.5 works with cuda 10.2, so lets install cuda 10.2. Below command will install cuda and nvidia-drivers inside <code>/usr/local/cuda-10.2</code> directory.</li>\n</ul>\n<pre><code class=\"hljs language-bash\">> sudo yum install cuda-10-2\n</code></pre>\n<ul>\n<li>By Now we know that only <code>/home</code> and <code>/apps</code> directory were shared across all other instances. So lets copy cuda directory from <code>/usr/local/</code> to <code>~/opt/cuda</code></li>\n</ul>\n<pre><code class=\"hljs language-bash\">> <span class=\"hljs-built_in\">mkdir</span> ~/opt/cuda\n> <span class=\"hljs-built_in\">cp</span> -r /usr/local/cuda-10.2/* ~/opt/cuda/\n</code></pre>\n<ul>\n<li>Export cuda path to Nvidia CUDA binary executables. Open the ~/.bashrc using your preferred text editor and add the following two lines</li>\n</ul>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">export</span> PATH=<span class=\"hljs-variable\">$HOME</span>/opt/cuda/bin:<span class=\"hljs-variable\">$PATH</span>\n<span class=\"hljs-built_in\">export</span> LD_LIBRARY_PATH=<span class=\"hljs-variable\">$HOME</span>/opt/cuda/lib64:<span class=\"hljs-variable\">$LD_LIBRARY_PATH</span>\n</code></pre>\n<ul>\n<li>Now Re-login or execute your updated ~/.bashrc file:</li>\n</ul>\n<pre><code class=\"hljs language-bash\">> <span class=\"hljs-built_in\">source</span> ~/.bashrc\n</code></pre>\n<ul>\n<li>Now Confirm the CUDA installation:</li>\n</ul>\n<pre><code class=\"hljs language-bash\">> nvcc --version\n> nvidia-smi <span class=\"hljs-comment\">#( doesn't work as we dont have gpu in login instances )</span>\n</code></pre>\n<blockquote>\n<p>By Now your slurm system is ready and can run any training script using <code>sbatch</code> scheduling. Below I will discuss on how to compile nvidia-apex library. It takes me 2 days just to make nvidia-apex work on slurm cluster.If you follow my above steps exaclty, I am sure you will be able to install nvidia apex library inside your slurm cluster without any problems.</p>\n</blockquote>\n<h1 id=\"installing-nvidia-apex-library-inside-your-slurm-cluster--optional-\">Installing Nvidia-Apex library inside your slurm cluster ( Optional )</h1>\n<p>If you don't know about Apex library, you can visit Nvidia-Apex <a href=\"https://github.com/NVIDIA/apex\">github page</a> to know more about Apex library. It is a set of utilites to help training model in mixed precision mode. It provide more numerical stable layer norm operations during training model in mixed precision mode, So that you can have both faster and stable training.</p>\n<p>For <code>Nvidia-apex</code> to sucessfully compile, You need to upgrade your gcc --version to 7.3.0 .</p>\n<pre><code class=\"hljs language-bash\">> sudo yum install centos-release-scl\n> sudo yum install devtoolset-7-gcc*\n> scl <span class=\"hljs-built_in\">enable</span> devtoolset-7 bash\n> <span class=\"hljs-built_in\">which</span> gcc\n> gcc --version\n(above bash script must output gcc version as 7.3.0)\n</code></pre>\n<p>Now We will compile and install Apex-library inside python3.8</p>\n<pre><code class=\"hljs language-bash\">\n> git <span class=\"hljs-built_in\">clone</span> https://github.com/NVIDIA/apex\n> <span class=\"hljs-built_in\">cd</span> apex\n> <span class=\"hljs-built_in\">export</span> TORCH_CUDA_ARCH_LIST=<span class=\"hljs-string\">\"6.0;7.0\"</span>\n> CUDA_HOME=<span class=\"hljs-variable\">$HOME</span>/opt/cuda python3.8 -m pip install -v --no-cache-dir --global-option=<span class=\"hljs-string\">\"--cpp_ext\"</span> --global-option=<span class=\"hljs-string\">\"--cuda_ext\"</span> ./\n</code></pre>\n<p>Its takes around 4 - 6 minutes to compile and install apex.</p>\n<h1 id=\"distributed-training-with-pytorch-and-slurm\">Distributed Training with pytorch and slurm</h1>\n<p>I will not give details on how to setup codebase in pytorch for distributed training, Its a huge topic and might need another blog post for it only. Meanwhile, You can learn more about distributed training with pytorch <a href=\"https://pytorch.org/tutorials/intermediate/dist_tuto.html\">here</a>. After you done setting up distributed training codebase in pytorch. You can start training using sbatch scripts.</p>\n<h2 id=\"running-the-training-job-using-sbatch-script\">Running the training Job Using SBATCH Script</h2>\n<p>After we have done the cluster setup, preparing deep learning envs and building of the model, the last step is to finally run a job, i.e. to start the training. That is easily done by running sbatch script, which is basically a customized shell script. It effectively has two parts. The first part of the script is specific for the Slurm, it specifies the parameters for the Slurm job scheduler using the SBATCH command. The second part consists of bash (or some other shell) commands that you would normally run in terminal.</p>\n<p>Below you will see demo SBATCH script (You need to modify sbatch script according to your needs).</p>\n<blockquote>\n<p>inside run_training.sh file.</p>\n</blockquote>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-meta\">#!/bin/sh</span>\n <span class=\"hljs-comment\">#SBATCH --job-name=distributed_training</span>\n  <span class=\"hljs-comment\">#SBATCH --output=slurm_logs/slrm_stdout.%j</span>\n  <span class=\"hljs-comment\">#SBATCH --error=slurm_logs/slrm_stderr.%j</span>\n  <span class=\"hljs-comment\">#SBATCH --partition=gpu</span>\n  <span class=\"hljs-comment\">## make sure we don't clobber log files if jobs get restarted</span>\n  <span class=\"hljs-comment\">#SBATCH --open-mode=append</span>\n  <span class=\"hljs-comment\">#SBATCH --nodes=2</span>\n  <span class=\"hljs-comment\">#SBATCH --time=24:00:00</span>\n  <span class=\"hljs-comment\">## make sure we are told about preempts, and jobs running out of time, 60s beforehand</span>\n  <span class=\"hljs-comment\">#SBATCH --signal=USR1@60</span>\n  <span class=\"hljs-comment\">#SBATCH --cpus-per-task=5</span>\n  <span class=\"hljs-comment\">## srun forks ntasks_per_node times on each node</span>\n  <span class=\"hljs-comment\">#SBATCH --ntasks-per-node=1</span>\n  <span class=\"hljs-comment\">#SBATCH --mem=200G</span>\n  <span class=\"hljs-comment\">#SBATCH --gpus-per-node=1</span>\n\npython3.8 train.py &#x3C;Your training args here>\n</code></pre>\n<p>Execute the sbatch script using the sbatch command line:</p>\n<pre><code class=\"hljs language-bash\">> sbatch run_training.sh\n</code></pre>\n<p>Running sbatch will return a Job ID for the scheduled job, for example:</p>\n<pre><code>Submitted batch job 37\n</code></pre>\n<p>To keep track of the job’s state, run squeue and to keep track of the cluster’s state, run sinfo:</p>\n<pre><code class=\"hljs language-bash\">> squeue\n\nJOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n3 gpu hostname &#x3C;username> CF 0:11 4 slurm-job-compute-0-[0-1]\n\n> sinfo\n\nPARTITION AVAIL TIMELIMIT NODES STATE NODELIST\ngpu* up infinite 4 mix<span class=\"hljs-comment\"># slurm-job-compute-0-[0-1]</span>\ngpu* up infinite 6 idle~ slurm-job-compute-0-[2-7]\n</code></pre>\n<h1 id=\"summary\">Summary</h1>\n<p>In this blog post we learned how to setup slurm cluster in GCP ( which is super easy ), setup own deeplearning environment from installing python, pytorch to compiling apex from source and finally run training using sbatch script and keeping track of job state using <code>sinfo</code>, <code>squeue</code> and <code>scontrol</code>.</p>\n<h1 id=\"references\">REFERENCES</h1>\n<ol>\n<li><a href=\"https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine\">https://cloud.google.com/solutions/deploying-slurm-cluster-compute-engine</a></li>\n<li><a href=\"https://cloud.google.com/deployment-manager/docs/deployments\">https://cloud.google.com/deployment-manager/docs/deployments</a></li>\n<li><a href=\"https://pytorch.org/tutorials/intermediate/dist_tuto.html\">https://pytorch.org/tutorials/intermediate/dist_tuto.html</a></li>\n<li><a href=\"https://github.com/NVIDIA/apex\">https://github.com/NVIDIA/apex</a></li>\n</ol>"}},"__N_SSG":true}